{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Inception_Resnet.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSA1Rl78a4Tk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca2eea2-54aa-4184-be69-f985ae3b6c7d"
      },
      "source": [
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/1a/0d79814736cfecc825ab8094b39648cc9c46af7af1bae839928acb73b4dd/tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 54.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.12.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 54.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.12.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.28.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (54.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPFCzc98XuOP",
        "outputId": "cc3aa37f-4dc3-4e7a-a61f-6ca9c4d9ad3f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT9-NfQX-em4"
      },
      "source": [
        "#Py Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWqlBmN08BYI",
        "outputId": "f8d60050-428b-4ecb-c3a7-2cb0f587afcc"
      },
      "source": [
        "!pip install kora\n",
        "# import kora.install.py38"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/pip: /usr/local/bin/pip3: /usr/bin/python3: bad interpreter: No such file or directory\n",
            "/usr/local/bin/pip: line 50: /usr/local/bin/pip3: Success\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqiD57NYDO6S"
      },
      "source": [
        "import kora.install.py38"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC5EMJ3kEeyN",
        "outputId": "91e352d4-9ffd-4b2d-db2d-99f68d73d1a6"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAO6ajnw9UYm",
        "outputId": "a5f274af-852e-412f-f52f-d146ab9f00a4"
      },
      "source": [
        "!sudo apt install python3.8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3.8-minimal\n",
            "Suggested packages:\n",
            "  python3.8-venv python3.8-doc binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3.8 python3.8-minimal\n",
            "0 upgraded, 4 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 4,669 kB of archives.\n",
            "After this operation, 18.5 MB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 libpython3.8-minimal amd64 3.8.9-1+bionic1 [761 kB]\n",
            "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.8-minimal amd64 3.8.9-1+bionic1 [1,823 kB]\n",
            "Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 libpython3.8-stdlib amd64 3.8.9-1+bionic1 [1,656 kB]\n",
            "Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.8 amd64 3.8.9-1+bionic1 [430 kB]\n",
            "Fetched 4,669 kB in 3s (1,846 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 160983 files and directories currently installed.)\n",
            "Preparing to unpack .../libpython3.8-minimal_3.8.9-1+bionic1_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.9-1+bionic1) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../python3.8-minimal_3.8.9-1+bionic1_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.9-1+bionic1) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../libpython3.8-stdlib_3.8.9-1+bionic1_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.9-1+bionic1) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../python3.8_3.8.9-1+bionic1_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.9-1+bionic1) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.9-1+bionic1) ...\n",
            "Setting up python3.8-minimal (3.8.9-1+bionic1) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.9-1+bionic1) ...\n",
            "Setting up python3.8 (3.8.9-1+bionic1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVXJ5Zj29e59",
        "outputId": "328352ec-702e-4195-f147-22f6d1fbbd6e"
      },
      "source": [
        "!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.7.10\n",
        "# !update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.8 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "update-alternatives: --install needs <link> <name> <path> <priority>\n",
            "\n",
            "Use 'update-alternatives --help' for program usage information.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okVM33m58MjG",
        "outputId": "13a82807-dfba-4bea-d2b5-c56030021c62"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqkdbx1Z-T2-",
        "outputId": "d78dc39e-b7ce-41d0-c842-d59cc1870903"
      },
      "source": [
        "# install Anaconda3\n",
        "!wget -qO ac.sh https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh \n",
        "# !wget -u ac.sh https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh\n",
        "!bash ./ac.sh -b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: File or directory already exists: '/root/anaconda3'\n",
            "If you want to update an existing installation, use the -u option.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qyGXCCv_W7U"
      },
      "source": [
        "# a fake google.colab library\n",
        "!ln -s /usr/local/lib/python3.7/dist-packages/google \\\n",
        "       /root/anaconda3/lib/python3.8/site-packages/google_colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQYG0VR_GkUe",
        "outputId": "e855ead7-126e-4495-eea0-2236e7b05551"
      },
      "source": [
        "# start jupyterlab, which now has Python3 = 3.8\n",
        "!nohup /root/anaconda3/bin/jupyter-lab --ip=0.0.0.0&"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YvSbWedGqCF",
        "outputId": "170f9b30-742a-4425-d9af-5bc18ca4a08c"
      },
      "source": [
        "!pip install pyngrok -q\n",
        "#from pyngrok import ngrok\n",
        "# print(ngrok.connect(8888)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/pip: /usr/local/bin/pip3: /usr/bin/python3: bad interpreter: No such file or directory\n",
            "/usr/local/bin/pip: line 50: /usr/local/bin/pip3: Success\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "HbGxdD1wGzDe",
        "outputId": "a6204525-ec78-4e40-9fa2-894ac52c0a01"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "print(ngrok.connect(8888)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t=2021-04-21T06:26:12+0000 lvl=warn msg=\"failed to start tunnel\" pg=/api/tunnels id=d65d294a7f76eac8 err=\"Your account may not run more than 2 tunnels over a single ngrok client session.\\nThe tunnels already running on this session are:\\n[]\\n\\r\\n\\r\\nERR_NGROK_324\\r\\n\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokHTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 502: Bad Gateway",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-38c501157a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8888\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     tunnel = NgrokTunnel(api_request(\"{}/api/tunnels\".format(api_url), method=\"POST\", data=options,\n\u001b[0;32m--> 256\u001b[0;31m                                      timeout=pyngrok_config.request_timeout),\n\u001b[0m\u001b[1;32m    257\u001b[0m                          pyngrok_config, api_url)\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout)\u001b[0m\n\u001b[1;32m    461\u001b[0m         raise PyngrokNgrokHTTPError(\"ngrok client exception, API returned {}: {}\".format(status_code, response_data),\n\u001b[1;32m    462\u001b[0m                                     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                                     status_code, e.msg, e.hdrs, response_data)\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mPyngrokNgrokURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ngrok client exception, URLError: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m: ngrok client exception, API returned 502: {\"error_code\":103,\"status_code\":502,\"msg\":\"failed to start tunnel\",\"details\":{\"err\":\"Your account may not run more than 2 tunnels over a single ngrok client session.\\nThe tunnels already running on this session are:\\n[]\\n\\r\\n\\r\\nERR_NGROK_324\\r\\n\"}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6O0tpICYBiWJ",
        "outputId": "79783ee9-1bc5-472b-814c-3e646faec00d"
      },
      "source": [
        "!apt-get upgrade python3.8\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3.8 is already the newest version (3.8.9-1+bionic1).\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn8 libcudnn8-dev libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  binutils binutils-common binutils-x86-64-linux-gnu cuda-compat-11-0 gnupg2\n",
            "  libaudit-common libaudit1 libbinutils libc-bin libcublas-dev libcublas10\n",
            "  libcudnn7 libcudnn7-dev libldap-2.4-2 libldap-common libp11-kit0 libperl5.26\n",
            "  libsasl2-2 libsasl2-modules-db libzstd1 linux-libc-dev openssl perl\n",
            "  perl-base perl-modules-5.26 tar ubuntu-keyring\n",
            "27 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 460 MB of archives.\n",
            "After this operation, 70.0 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-compat-11-0 450.102.04-1 [6,712 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libperl5.26 amd64 5.26.1-6ubuntu0.5 [3,534 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas10 10.2.3.254-1 [43.1 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 perl amd64 5.26.1-6ubuntu0.5 [201 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 perl-base amd64 5.26.1-6ubuntu0.5 [1,391 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 perl-modules-5.26 all 5.26.1-6ubuntu0.5 [2,762 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas-dev 10.2.3.254-1 [42.4 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 tar amd64 1.29b-2ubuntu0.2 [234 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc-bin amd64 2.27-3ubuntu1.4 [643 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaudit-common all 1:2.8.2-1ubuntu1.1 [4,068 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaudit1 amd64 1:2.8.2-1ubuntu1.1 [38.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzstd1 amd64 1.3.3+dfsg-2ubuntu1.2 [189 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libp11-kit0 amd64 0.23.9-2ubuntu0.1 [187 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ubuntu-keyring all 2018.09.18.1~18.04.2 [22.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssl amd64 1.1.1-1ubuntu2.1~18.04.9 [614 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.30-21ubuntu1~18.04.5 [1,839 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-common amd64 2.30-21ubuntu1~18.04.5 [197 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils amd64 2.30-21ubuntu1~18.04.5 [3,388 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbinutils amd64 2.30-21ubuntu1~18.04.5 [489 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-modules-db amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.3 [15.0 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-2 amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.3 [49.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-common all 2.4.45+dfsg-1ubuntu1.10 [15.8 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-2.4-2 amd64 2.4.45+dfsg-1ubuntu1.10 [154 kB]\n",
            "Ign:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-140.144\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 gnupg2 all 2.2.4-1ubuntu1.4 [5,292 B]\n",
            "Err:24 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-140.144\n",
            "  404  Not Found [IP: 91.189.88.142 80]\n",
            "Get:26 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libcudnn7-dev 7.6.5.32-1+cuda10.2 [165 MB]\n",
            "Get:27 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libcudnn7 7.6.5.32-1+cuda10.2 [189 MB]\n",
            "Fetched 459 MB in 7s (61.3 MB/s)\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/l/linux/linux-libc-dev_4.15.0-140.144_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3PYXvA2CFfD",
        "outputId": "785504c7-fbc7-49b5-c80d-49ef2dd67daf"
      },
      "source": [
        "! apt-get update"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,116 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.5 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,410 kB]\n",
            "Get:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [395 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [739 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,756 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.4 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [425 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,182 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [899 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,546 kB]\n",
            "Get:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [39.5 kB]\n",
            "Fetched 12.9 MB in 3s (4,523 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoB_wt8VIILv",
        "outputId": "989c8faa-3078-4a77-d6f3-506ef97451e1"
      },
      "source": [
        "!apt-get install python3.8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3.8 is already the newest version (3.8.9-1+bionic1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 78 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWbDD-JIBmw1",
        "outputId": "c98e5ddc-3e61-4e75-ca7f-83fbbd0631b2"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuS6u3Og-bMY"
      },
      "source": [
        "#Continue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvVHs1L1a4Tv"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YMhsO_sa4Tw"
      },
      "source": [
        "\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft1Lbek7a4Tx"
      },
      "source": [
        "# re-size all the images to this\n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "train_path = '/content/drive/MyDrive/Horizontal_bar_data/Training_Set'\n",
        "valid_path = '/content/drive/MyDrive/Horizontal_bar_data/Test_Set'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujnR7e4Na4Tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27fae8da-14cd-4cc7-c452-f40045c2f322"
      },
      "source": [
        "# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n",
        "# Here we will be using imagenet weights\n",
        "#inception = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "# resnet = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "inception_resnet = InceptionResNetV2(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68QfLLVEa4Ty"
      },
      "source": [
        "\n",
        "\n",
        "# don't train existing weights\n",
        "# for layer in resnet.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "for layer in inception_resnet.layers:\n",
        "     layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPsByg9La4Ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130d6d63-85ed-443d-8983-71dcdda453b0"
      },
      "source": [
        "from glob import glob\n",
        "# useful for getting number of output classes\n",
        "#folders = glob('Datasets/train/*')\n",
        "#Downloads/archive(1)/dataset/training_set\n",
        "folders = glob('/content/drive/MyDrive/Horizontal_bar_data/Training_Set/*') \n",
        "#folders = glob('Downloads/archive(1)/dog vs cat/dataset/training_set/?')\n",
        "\n",
        "folders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Horizontal_bar_data/Training_Set/Grouped',\n",
              " '/content/drive/MyDrive/Horizontal_bar_data/Training_Set/Simple',\n",
              " '/content/drive/MyDrive/Horizontal_bar_data/Training_Set/Stacked']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnIE7ZjZa4Tz",
        "outputId": "6aeaa36b-bf1b-4740-9bac-5e07230a675c"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/karthik/Downloads\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKexjCgOa4T0",
        "outputId": "395a6fcb-ce40-4e23-ce49-5bc66b922593"
      },
      "source": [
        "import os\n",
        "data_dir = 'Downloads/archive(1)/dataset/training_set'\n",
        "print('Folders :', os.listdir(data_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Downloads/archive(1)/dataset/training_set'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d08d0b97be7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Downloads/archive(1)/dataset/training_set'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Folders :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Downloads/archive(1)/dataset/training_set'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHOhSJxWa4T0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a58d682-6a90-4f08-e7f1-c670470237ae"
      },
      "source": [
        "tf.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/version/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z84b3_tfa4T1",
        "outputId": "ac62041f-0b17-44d8-8088-100f9ba37a9c"
      },
      "source": [
        "folders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQI1atzPa4T1"
      },
      "source": [
        "\n",
        "\n",
        "# our layers - you can add more if you want\n",
        "x = Flatten()(inception_resnet.output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T0Vc51ca4T2"
      },
      "source": [
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "\n",
        "# create a model object\n",
        "model = Model(inputs=inception_resnet.input, outputs=prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU9eL2AYa4T2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7e9eee-61b4-40a9-ce91-78d12b481784"
      },
      "source": [
        "# view the structure of the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 111, 111, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 111, 111, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 111, 111, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 109, 109, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 109, 109, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 109, 109, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 109, 109, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 109, 109, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 109, 109, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 54, 54, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 54, 54, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 54, 54, 80)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 52, 52, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 52, 52, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 52, 52, 192)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 25, 25, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 25, 25, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 25, 25, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 25, 25, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 25, 25, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 25, 25, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 25, 25, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 25, 25, 96)   18432       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 25, 25, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 25, 25, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 25, 25, 64)   12288       average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 25, 25, 96)   288         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 25, 25, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 25, 25, 64)   192         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 25, 25, 96)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 25, 25, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 25, 25, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed_5b (Concatenate)          (None, 25, 25, 320)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 25, 25, 32)   10240       mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 25, 25, 32)   96          conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 25, 25, 32)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 25, 25, 32)   10240       mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 25, 25, 48)   13824       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 25, 25, 32)   96          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 25, 25, 48)   144         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 25, 25, 32)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 25, 25, 48)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 25, 25, 32)   10240       mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 25, 25, 32)   9216        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 25, 25, 64)   27648       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 25, 25, 32)   96          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 25, 25, 32)   96          conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 25, 25, 64)   192         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 25, 25, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 25, 25, 32)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 25, 25, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_1_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_1_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_1_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_1 (Lambda)              (None, 25, 25, 320)  0           mixed_5b[0][0]                   \n",
            "                                                                 block35_1_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_1_ac (Activation)       (None, 25, 25, 320)  0           block35_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 25, 25, 32)   10240       block35_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 25, 25, 32)   96          conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 25, 25, 32)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 25, 25, 32)   10240       block35_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 25, 25, 48)   13824       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 25, 25, 32)   96          conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 25, 25, 48)   144         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 25, 25, 32)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 25, 25, 48)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 25, 25, 32)   10240       block35_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 25, 25, 32)   9216        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 25, 25, 64)   27648       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 25, 25, 32)   96          conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 25, 25, 32)   96          conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 25, 25, 64)   192         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 25, 25, 32)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 25, 25, 32)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 25, 25, 64)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_2_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_18[0][0]              \n",
            "                                                                 activation_20[0][0]              \n",
            "                                                                 activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_2_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_2_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_2 (Lambda)              (None, 25, 25, 320)  0           block35_1_ac[0][0]               \n",
            "                                                                 block35_2_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_2_ac (Activation)       (None, 25, 25, 320)  0           block35_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 25, 25, 32)   10240       block35_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 25, 25, 32)   96          conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 25, 25, 32)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 25, 25, 32)   10240       block35_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 25, 25, 48)   13824       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 25, 25, 32)   96          conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 25, 25, 48)   144         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 25, 25, 32)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 25, 25, 48)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 25, 25, 32)   10240       block35_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 25, 25, 32)   9216        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 25, 25, 64)   27648       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 25, 25, 32)   96          conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 25, 25, 32)   96          conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 25, 25, 64)   192         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 25, 25, 32)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 25, 25, 32)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 25, 25, 64)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_3_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_24[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_3_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_3_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_3 (Lambda)              (None, 25, 25, 320)  0           block35_2_ac[0][0]               \n",
            "                                                                 block35_3_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_3_ac (Activation)       (None, 25, 25, 320)  0           block35_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 25, 25, 32)   10240       block35_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 25, 25, 32)   96          conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 25, 25, 32)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 25, 25, 32)   10240       block35_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 25, 25, 48)   13824       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 25, 25, 32)   96          conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 25, 25, 48)   144         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 25, 25, 32)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 25, 25, 48)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 25, 25, 32)   10240       block35_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 25, 25, 32)   9216        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 25, 25, 64)   27648       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 25, 25, 32)   96          conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 25, 25, 32)   96          conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 25, 25, 64)   192         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 25, 25, 32)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 25, 25, 32)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 25, 25, 64)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_4_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_30[0][0]              \n",
            "                                                                 activation_32[0][0]              \n",
            "                                                                 activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_4_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_4_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_4 (Lambda)              (None, 25, 25, 320)  0           block35_3_ac[0][0]               \n",
            "                                                                 block35_4_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_4_ac (Activation)       (None, 25, 25, 320)  0           block35_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 25, 25, 32)   10240       block35_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 25, 25, 32)   96          conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 25, 25, 32)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 25, 25, 32)   10240       block35_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 25, 25, 48)   13824       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 25, 25, 32)   96          conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 25, 25, 48)   144         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 25, 25, 32)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 25, 25, 48)   0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 25, 25, 32)   10240       block35_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 25, 25, 32)   9216        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 25, 25, 64)   27648       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 25, 25, 32)   96          conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 25, 25, 32)   96          conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 25, 25, 64)   192         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 25, 25, 32)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 25, 25, 32)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 25, 25, 64)   0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_5_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_36[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_5_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_5_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_5 (Lambda)              (None, 25, 25, 320)  0           block35_4_ac[0][0]               \n",
            "                                                                 block35_5_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_5_ac (Activation)       (None, 25, 25, 320)  0           block35_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 25, 25, 32)   10240       block35_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 25, 25, 32)   96          conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 25, 25, 32)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 25, 25, 32)   10240       block35_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 25, 25, 48)   13824       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 25, 25, 32)   96          conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 25, 25, 48)   144         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 25, 25, 32)   0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 25, 25, 48)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 25, 25, 32)   10240       block35_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 25, 25, 32)   9216        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 25, 25, 64)   27648       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 25, 25, 32)   96          conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 25, 25, 32)   96          conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 25, 25, 64)   192         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 25, 25, 32)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 25, 25, 32)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 25, 25, 64)   0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_6_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_42[0][0]              \n",
            "                                                                 activation_44[0][0]              \n",
            "                                                                 activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_6_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_6_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_6 (Lambda)              (None, 25, 25, 320)  0           block35_5_ac[0][0]               \n",
            "                                                                 block35_6_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_6_ac (Activation)       (None, 25, 25, 320)  0           block35_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 25, 25, 32)   10240       block35_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 25, 25, 32)   96          conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 25, 25, 32)   0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 25, 25, 32)   10240       block35_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 25, 25, 48)   13824       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 25, 25, 32)   96          conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 25, 25, 48)   144         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 25, 25, 32)   0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 25, 25, 48)   0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 25, 25, 32)   10240       block35_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 25, 25, 32)   9216        activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 25, 25, 64)   27648       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 25, 25, 32)   96          conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 25, 25, 32)   96          conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 25, 25, 64)   192         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 25, 25, 32)   0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 25, 25, 32)   0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 25, 25, 64)   0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_7_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_48[0][0]              \n",
            "                                                                 activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_7_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_7_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_7 (Lambda)              (None, 25, 25, 320)  0           block35_6_ac[0][0]               \n",
            "                                                                 block35_7_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_7_ac (Activation)       (None, 25, 25, 320)  0           block35_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 25, 25, 32)   10240       block35_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 25, 25, 32)   96          conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 25, 25, 32)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 25, 25, 32)   10240       block35_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 25, 25, 48)   13824       activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 25, 25, 32)   96          conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 25, 25, 48)   144         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 25, 25, 32)   0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 25, 25, 48)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 25, 25, 32)   10240       block35_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 25, 25, 32)   9216        activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 25, 25, 64)   27648       activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 25, 25, 32)   96          conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 25, 25, 32)   96          conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 25, 25, 64)   192         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 25, 25, 32)   0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 25, 25, 32)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 25, 25, 64)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_8_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_54[0][0]              \n",
            "                                                                 activation_56[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_8_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_8_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_8 (Lambda)              (None, 25, 25, 320)  0           block35_7_ac[0][0]               \n",
            "                                                                 block35_8_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_8_ac (Activation)       (None, 25, 25, 320)  0           block35_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 25, 25, 32)   10240       block35_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 25, 25, 32)   96          conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 25, 25, 32)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 25, 25, 32)   10240       block35_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 25, 25, 48)   13824       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 25, 25, 32)   96          conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 25, 25, 48)   144         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 25, 25, 32)   0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 25, 25, 48)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 25, 25, 32)   10240       block35_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 25, 25, 32)   9216        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 25, 25, 64)   27648       activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 25, 25, 32)   96          conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 25, 25, 32)   96          conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 25, 25, 64)   192         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 25, 25, 32)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 25, 25, 32)   0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 25, 25, 64)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_9_mixed (Concatenate)   (None, 25, 25, 128)  0           activation_60[0][0]              \n",
            "                                                                 activation_62[0][0]              \n",
            "                                                                 activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_9_conv (Conv2D)         (None, 25, 25, 320)  41280       block35_9_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_9 (Lambda)              (None, 25, 25, 320)  0           block35_8_ac[0][0]               \n",
            "                                                                 block35_9_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block35_9_ac (Activation)       (None, 25, 25, 320)  0           block35_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 25, 25, 32)   10240       block35_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 25, 25, 32)   96          conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 25, 25, 32)   0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 25, 25, 32)   10240       block35_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 25, 25, 48)   13824       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 25, 25, 32)   96          conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 25, 25, 48)   144         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 25, 25, 32)   0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 25, 25, 48)   0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 25, 25, 32)   10240       block35_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 25, 25, 32)   9216        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 25, 25, 64)   27648       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 25, 25, 32)   96          conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 25, 25, 32)   96          conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 25, 25, 64)   192         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 25, 25, 32)   0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 25, 25, 32)   0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 25, 25, 64)   0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block35_10_mixed (Concatenate)  (None, 25, 25, 128)  0           activation_66[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block35_10_conv (Conv2D)        (None, 25, 25, 320)  41280       block35_10_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block35_10 (Lambda)             (None, 25, 25, 320)  0           block35_9_ac[0][0]               \n",
            "                                                                 block35_10_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block35_10_ac (Activation)      (None, 25, 25, 320)  0           block35_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 25, 25, 256)  81920       block35_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 25, 25, 256)  768         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 25, 25, 256)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 25, 25, 256)  589824      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 25, 25, 256)  768         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 25, 25, 256)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 12, 12, 384)  1105920     block35_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 12, 12, 384)  884736      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 12, 12, 384)  1152        conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 12, 12, 384)  1152        conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 12, 12, 384)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 12, 12, 384)  0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 320)  0           block35_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "mixed_6a (Concatenate)          (None, 12, 12, 1088) 0           activation_72[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 12, 12, 128)  139264      mixed_6a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 12, 12, 128)  384         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 12, 12, 128)  0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 12, 12, 160)  143360      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 12, 12, 160)  480         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 12, 12, 160)  0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 12, 12, 192)  208896      mixed_6a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 12, 12, 192)  215040      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 12, 12, 192)  576         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 12, 12, 192)  576         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 12, 12, 192)  0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 12, 12, 192)  0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_1_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_76[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_1_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_1_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_1 (Lambda)              (None, 12, 12, 1088) 0           mixed_6a[0][0]                   \n",
            "                                                                 block17_1_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_1_ac (Activation)       (None, 12, 12, 1088) 0           block17_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 12, 12, 128)  139264      block17_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 12, 12, 128)  384         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 12, 12, 128)  0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 12, 12, 160)  143360      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 12, 12, 160)  480         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 12, 12, 160)  0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 12, 12, 192)  208896      block17_1_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 12, 12, 192)  215040      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 12, 12, 192)  576         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 12, 12, 192)  576         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 12, 12, 192)  0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 12, 12, 192)  0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_2_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_80[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_2_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_2_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_2 (Lambda)              (None, 12, 12, 1088) 0           block17_1_ac[0][0]               \n",
            "                                                                 block17_2_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_2_ac (Activation)       (None, 12, 12, 1088) 0           block17_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 12, 12, 128)  139264      block17_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 12, 12, 128)  384         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 12, 12, 128)  0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 12, 12, 160)  143360      activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 12, 12, 160)  480         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 12, 12, 160)  0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 12, 12, 192)  208896      block17_2_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 12, 12, 192)  215040      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 12, 12, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 12, 12, 192)  576         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 12, 12, 192)  0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 12, 12, 192)  0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_3_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_84[0][0]              \n",
            "                                                                 activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_3_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_3_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_3 (Lambda)              (None, 12, 12, 1088) 0           block17_2_ac[0][0]               \n",
            "                                                                 block17_3_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_3_ac (Activation)       (None, 12, 12, 1088) 0           block17_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 12, 12, 128)  139264      block17_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 12, 12, 128)  384         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 12, 12, 128)  0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 12, 12, 160)  143360      activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 12, 12, 160)  480         conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 12, 12, 160)  0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 12, 12, 192)  208896      block17_3_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 12, 12, 192)  215040      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 12, 12, 192)  576         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 12, 12, 192)  576         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 12, 12, 192)  0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 12, 12, 192)  0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_4_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_88[0][0]              \n",
            "                                                                 activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_4_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_4_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_4 (Lambda)              (None, 12, 12, 1088) 0           block17_3_ac[0][0]               \n",
            "                                                                 block17_4_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_4_ac (Activation)       (None, 12, 12, 1088) 0           block17_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 12, 12, 128)  139264      block17_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 12, 12, 128)  384         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 12, 12, 128)  0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 12, 12, 160)  143360      activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 12, 12, 160)  480         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 12, 12, 160)  0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 12, 12, 192)  208896      block17_4_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 12, 12, 192)  215040      activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 12, 12, 192)  576         conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 12, 12, 192)  576         conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 12, 12, 192)  0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 12, 12, 192)  0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_5_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_92[0][0]              \n",
            "                                                                 activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_5_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_5_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_5 (Lambda)              (None, 12, 12, 1088) 0           block17_4_ac[0][0]               \n",
            "                                                                 block17_5_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_5_ac (Activation)       (None, 12, 12, 1088) 0           block17_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 12, 12, 128)  139264      block17_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 12, 12, 128)  384         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 12, 12, 128)  0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 12, 12, 160)  143360      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 12, 12, 160)  480         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 12, 12, 160)  0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 12, 12, 192)  208896      block17_5_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 12, 12, 192)  215040      activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 12, 12, 192)  576         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 12, 12, 192)  576         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 12, 12, 192)  0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 12, 12, 192)  0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block17_6_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_96[0][0]              \n",
            "                                                                 activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block17_6_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_6_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_6 (Lambda)              (None, 12, 12, 1088) 0           block17_5_ac[0][0]               \n",
            "                                                                 block17_6_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_6_ac (Activation)       (None, 12, 12, 1088) 0           block17_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 12, 12, 128)  139264      block17_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 12, 12, 128)  384         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 12, 12, 128)  0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 12, 12, 160)  143360      activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 12, 12, 160)  480         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 12, 12, 160)  0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 12, 12, 192)  208896      block17_6_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 12, 12, 192)  215040      activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 12, 12, 192)  576         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 12, 12, 192)  576         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 12, 12, 192)  0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 12, 12, 192)  0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_7_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_100[0][0]             \n",
            "                                                                 activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_7_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_7_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_7 (Lambda)              (None, 12, 12, 1088) 0           block17_6_ac[0][0]               \n",
            "                                                                 block17_7_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_7_ac (Activation)       (None, 12, 12, 1088) 0           block17_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 12, 12, 128)  139264      block17_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 12, 12, 128)  384         conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 12, 12, 128)  0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 12, 12, 160)  143360      activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 12, 12, 160)  480         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 12, 12, 160)  0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 12, 12, 192)  208896      block17_7_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 12, 12, 192)  215040      activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 12, 12, 192)  576         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 12, 12, 192)  576         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 12, 12, 192)  0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 12, 12, 192)  0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_8_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_104[0][0]             \n",
            "                                                                 activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_8_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_8_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_8 (Lambda)              (None, 12, 12, 1088) 0           block17_7_ac[0][0]               \n",
            "                                                                 block17_8_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_8_ac (Activation)       (None, 12, 12, 1088) 0           block17_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 12, 12, 128)  139264      block17_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 12, 12, 128)  384         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 12, 12, 128)  0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 12, 12, 160)  143360      activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 12, 12, 160)  480         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 12, 12, 160)  0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 12, 12, 192)  208896      block17_8_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 12, 12, 192)  215040      activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 12, 12, 192)  576         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 12, 12, 192)  576         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 12, 12, 192)  0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 12, 12, 192)  0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_9_mixed (Concatenate)   (None, 12, 12, 384)  0           activation_108[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_9_conv (Conv2D)         (None, 12, 12, 1088) 418880      block17_9_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_9 (Lambda)              (None, 12, 12, 1088) 0           block17_8_ac[0][0]               \n",
            "                                                                 block17_9_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_9_ac (Activation)       (None, 12, 12, 1088) 0           block17_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 12, 12, 128)  139264      block17_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 12, 12, 128)  384         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 12, 12, 128)  0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 12, 12, 160)  143360      activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 12, 12, 160)  480         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 12, 12, 160)  0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 12, 12, 192)  208896      block17_9_ac[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 12, 12, 192)  215040      activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 12, 12, 192)  576         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 12, 12, 192)  576         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 12, 12, 192)  0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 12, 12, 192)  0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_10_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_112[0][0]             \n",
            "                                                                 activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_10_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_10_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_10 (Lambda)             (None, 12, 12, 1088) 0           block17_9_ac[0][0]               \n",
            "                                                                 block17_10_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_10_ac (Activation)      (None, 12, 12, 1088) 0           block17_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 12, 12, 128)  139264      block17_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 12, 12, 128)  384         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 12, 12, 128)  0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 12, 12, 160)  143360      activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 12, 12, 160)  480         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 12, 12, 160)  0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 12, 12, 192)  208896      block17_10_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 12, 12, 192)  215040      activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 12, 12, 192)  576         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 12, 12, 192)  576         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 12, 12, 192)  0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 12, 12, 192)  0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_11_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_116[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_11_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_11_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_11 (Lambda)             (None, 12, 12, 1088) 0           block17_10_ac[0][0]              \n",
            "                                                                 block17_11_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_11_ac (Activation)      (None, 12, 12, 1088) 0           block17_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 12, 12, 128)  139264      block17_11_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 12, 12, 128)  384         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 12, 12, 128)  0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 12, 12, 160)  143360      activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 12, 12, 160)  480         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 12, 12, 160)  0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 12, 12, 192)  208896      block17_11_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 12, 12, 192)  215040      activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 12, 12, 192)  576         conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 12, 12, 192)  576         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 12, 12, 192)  0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 12, 12, 192)  0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_12_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_120[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_12_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_12_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_12 (Lambda)             (None, 12, 12, 1088) 0           block17_11_ac[0][0]              \n",
            "                                                                 block17_12_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_12_ac (Activation)      (None, 12, 12, 1088) 0           block17_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 12, 12, 128)  139264      block17_12_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 12, 12, 128)  384         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 12, 12, 128)  0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 12, 12, 160)  143360      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 12, 12, 160)  480         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 12, 12, 160)  0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 12, 12, 192)  208896      block17_12_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 12, 12, 192)  215040      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 12, 12, 192)  576         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 12, 12, 192)  576         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 12, 12, 192)  0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 12, 12, 192)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_13_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_124[0][0]             \n",
            "                                                                 activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_13_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_13_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_13 (Lambda)             (None, 12, 12, 1088) 0           block17_12_ac[0][0]              \n",
            "                                                                 block17_13_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_13_ac (Activation)      (None, 12, 12, 1088) 0           block17_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 12, 12, 128)  139264      block17_13_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 12, 12, 128)  384         conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 12, 12, 128)  0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 12, 12, 160)  143360      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 12, 12, 160)  480         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 12, 12, 160)  0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 12, 12, 192)  208896      block17_13_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 12, 12, 192)  215040      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 12, 12, 192)  576         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 12, 12, 192)  576         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 12, 12, 192)  0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 12, 12, 192)  0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_14_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_128[0][0]             \n",
            "                                                                 activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_14_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_14_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_14 (Lambda)             (None, 12, 12, 1088) 0           block17_13_ac[0][0]              \n",
            "                                                                 block17_14_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_14_ac (Activation)      (None, 12, 12, 1088) 0           block17_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 12, 12, 128)  139264      block17_14_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 12, 12, 128)  384         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 12, 12, 128)  0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 12, 12, 160)  143360      activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 12, 12, 160)  480         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 12, 12, 160)  0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 12, 12, 192)  208896      block17_14_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 12, 12, 192)  215040      activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 12, 12, 192)  576         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 12, 12, 192)  576         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 12, 12, 192)  0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 12, 12, 192)  0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_15_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_132[0][0]             \n",
            "                                                                 activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_15_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_15_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_15 (Lambda)             (None, 12, 12, 1088) 0           block17_14_ac[0][0]              \n",
            "                                                                 block17_15_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_15_ac (Activation)      (None, 12, 12, 1088) 0           block17_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 12, 12, 128)  139264      block17_15_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 12, 12, 128)  384         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 12, 12, 128)  0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 12, 12, 160)  143360      activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 12, 12, 160)  480         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 12, 12, 160)  0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 12, 12, 192)  208896      block17_15_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 12, 12, 192)  215040      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 12, 12, 192)  576         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 12, 12, 192)  576         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 12, 12, 192)  0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 12, 12, 192)  0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_16_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_136[0][0]             \n",
            "                                                                 activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_16_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_16_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_16 (Lambda)             (None, 12, 12, 1088) 0           block17_15_ac[0][0]              \n",
            "                                                                 block17_16_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_16_ac (Activation)      (None, 12, 12, 1088) 0           block17_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 12, 12, 128)  139264      block17_16_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 12, 12, 128)  384         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 12, 12, 128)  0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 12, 12, 160)  143360      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 12, 12, 160)  480         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 12, 12, 160)  0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 12, 12, 192)  208896      block17_16_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 12, 12, 192)  215040      activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 12, 12, 192)  576         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 12, 12, 192)  576         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 12, 12, 192)  0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 12, 12, 192)  0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_17_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_140[0][0]             \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_17_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_17_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_17 (Lambda)             (None, 12, 12, 1088) 0           block17_16_ac[0][0]              \n",
            "                                                                 block17_17_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_17_ac (Activation)      (None, 12, 12, 1088) 0           block17_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 12, 12, 128)  139264      block17_17_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 12, 12, 128)  384         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 12, 12, 128)  0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 12, 12, 160)  143360      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 12, 12, 160)  480         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 12, 12, 160)  0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 12, 12, 192)  208896      block17_17_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 12, 12, 192)  215040      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 12, 12, 192)  576         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 12, 12, 192)  576         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 12, 12, 192)  0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 12, 12, 192)  0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_18_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_144[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_18_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_18_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_18 (Lambda)             (None, 12, 12, 1088) 0           block17_17_ac[0][0]              \n",
            "                                                                 block17_18_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_18_ac (Activation)      (None, 12, 12, 1088) 0           block17_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 12, 12, 128)  139264      block17_18_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 12, 12, 128)  384         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 12, 12, 128)  0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 12, 12, 160)  143360      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 12, 12, 160)  480         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 12, 12, 160)  0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 12, 12, 192)  208896      block17_18_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 12, 12, 192)  215040      activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 12, 12, 192)  576         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 12, 12, 192)  576         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 12, 12, 192)  0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 12, 12, 192)  0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_19_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_148[0][0]             \n",
            "                                                                 activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_19_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_19_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_19 (Lambda)             (None, 12, 12, 1088) 0           block17_18_ac[0][0]              \n",
            "                                                                 block17_19_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_19_ac (Activation)      (None, 12, 12, 1088) 0           block17_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 12, 12, 128)  139264      block17_19_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 12, 12, 128)  384         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 12, 12, 128)  0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 12, 12, 160)  143360      activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 12, 12, 160)  480         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 12, 12, 160)  0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 12, 12, 192)  208896      block17_19_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 12, 12, 192)  215040      activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 12, 12, 192)  576         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 12, 12, 192)  576         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 12, 12, 192)  0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 12, 12, 192)  0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block17_20_mixed (Concatenate)  (None, 12, 12, 384)  0           activation_152[0][0]             \n",
            "                                                                 activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block17_20_conv (Conv2D)        (None, 12, 12, 1088) 418880      block17_20_mixed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block17_20 (Lambda)             (None, 12, 12, 1088) 0           block17_19_ac[0][0]              \n",
            "                                                                 block17_20_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block17_20_ac (Activation)      (None, 12, 12, 1088) 0           block17_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 12, 12, 256)  278528      block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 12, 12, 256)  768         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 12, 12, 256)  0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 12, 12, 256)  278528      block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 12, 12, 256)  278528      block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 12, 12, 288)  663552      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 12, 12, 256)  768         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 12, 12, 256)  768         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 12, 12, 288)  864         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 12, 12, 256)  0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 12, 12, 256)  0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 12, 12, 288)  0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 5, 5, 384)    884736      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 5, 5, 288)    663552      activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 5, 5, 320)    829440      activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 5, 5, 384)    1152        conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 5, 5, 288)    864         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 5, 5, 320)    960         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 5, 5, 384)    0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 5, 5, 288)    0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 5, 5, 320)    0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 1088)   0           block17_20_ac[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "mixed_7a (Concatenate)          (None, 5, 5, 2080)   0           activation_157[0][0]             \n",
            "                                                                 activation_159[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 5, 5, 192)    399360      mixed_7a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 5, 5, 192)    576         conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 5, 5, 192)    0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 5, 5, 224)    129024      activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 5, 5, 224)    672         conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 5, 5, 224)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 5, 5, 192)    399360      mixed_7a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 5, 5, 256)    172032      activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 5, 5, 192)    576         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 5, 5, 256)    768         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 5, 5, 192)    0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 5, 5, 256)    0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_1_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_163[0][0]             \n",
            "                                                                 activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_1_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_1_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_1 (Lambda)               (None, 5, 5, 2080)   0           mixed_7a[0][0]                   \n",
            "                                                                 block8_1_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_1_ac (Activation)        (None, 5, 5, 2080)   0           block8_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 5, 5, 192)    399360      block8_1_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 5, 5, 192)    576         conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 5, 5, 192)    0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 5, 5, 224)    129024      activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 5, 5, 224)    672         conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 5, 5, 224)    0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 5, 5, 192)    399360      block8_1_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 5, 5, 256)    172032      activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 5, 5, 192)    576         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 5, 5, 256)    768         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 5, 5, 192)    0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 5, 5, 256)    0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_2_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_167[0][0]             \n",
            "                                                                 activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_2_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_2_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_2 (Lambda)               (None, 5, 5, 2080)   0           block8_1_ac[0][0]                \n",
            "                                                                 block8_2_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_2_ac (Activation)        (None, 5, 5, 2080)   0           block8_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 5, 5, 192)    399360      block8_2_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 5, 5, 192)    576         conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 5, 5, 192)    0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 5, 5, 224)    129024      activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 5, 5, 224)    672         conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 5, 5, 224)    0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 5, 5, 192)    399360      block8_2_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 5, 5, 256)    172032      activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 5, 5, 192)    576         conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 5, 5, 256)    768         conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 5, 5, 192)    0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 5, 5, 256)    0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_3_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_171[0][0]             \n",
            "                                                                 activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_3_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_3_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_3 (Lambda)               (None, 5, 5, 2080)   0           block8_2_ac[0][0]                \n",
            "                                                                 block8_3_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_3_ac (Activation)        (None, 5, 5, 2080)   0           block8_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 5, 5, 192)    399360      block8_3_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 5, 5, 192)    576         conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 5, 5, 192)    0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 5, 5, 224)    129024      activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 5, 5, 224)    672         conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 5, 5, 224)    0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 5, 5, 192)    399360      block8_3_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 5, 5, 256)    172032      activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 5, 5, 192)    576         conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 5, 5, 256)    768         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 5, 5, 192)    0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 5, 5, 256)    0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_4_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_175[0][0]             \n",
            "                                                                 activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_4_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_4_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_4 (Lambda)               (None, 5, 5, 2080)   0           block8_3_ac[0][0]                \n",
            "                                                                 block8_4_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_4_ac (Activation)        (None, 5, 5, 2080)   0           block8_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 5, 5, 192)    399360      block8_4_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 5, 5, 192)    576         conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 5, 5, 192)    0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 5, 5, 224)    129024      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 5, 5, 224)    672         conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 5, 5, 224)    0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 5, 5, 192)    399360      block8_4_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 5, 5, 256)    172032      activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 5, 5, 192)    576         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 5, 5, 256)    768         conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 5, 5, 192)    0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 5, 5, 256)    0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_5_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_179[0][0]             \n",
            "                                                                 activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_5_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_5_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_5 (Lambda)               (None, 5, 5, 2080)   0           block8_4_ac[0][0]                \n",
            "                                                                 block8_5_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_5_ac (Activation)        (None, 5, 5, 2080)   0           block8_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 5, 5, 192)    399360      block8_5_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 5, 5, 192)    576         conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 5, 5, 192)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 5, 5, 224)    129024      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 5, 5, 224)    672         conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 5, 5, 224)    0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 5, 5, 192)    399360      block8_5_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 5, 5, 256)    172032      activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 5, 5, 192)    576         conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 5, 5, 256)    768         conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 5, 5, 192)    0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 5, 5, 256)    0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_6_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_183[0][0]             \n",
            "                                                                 activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_6_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_6_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_6 (Lambda)               (None, 5, 5, 2080)   0           block8_5_ac[0][0]                \n",
            "                                                                 block8_6_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_6_ac (Activation)        (None, 5, 5, 2080)   0           block8_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 5, 5, 192)    399360      block8_6_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 5, 5, 192)    576         conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 5, 5, 192)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 5, 5, 224)    129024      activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 5, 5, 224)    672         conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 5, 5, 224)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 5, 5, 192)    399360      block8_6_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 5, 5, 256)    172032      activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 5, 5, 192)    576         conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 5, 5, 256)    768         conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 5, 5, 192)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 5, 5, 256)    0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_7_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_187[0][0]             \n",
            "                                                                 activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_7_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_7_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_7 (Lambda)               (None, 5, 5, 2080)   0           block8_6_ac[0][0]                \n",
            "                                                                 block8_7_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_7_ac (Activation)        (None, 5, 5, 2080)   0           block8_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 5, 5, 192)    399360      block8_7_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 5, 5, 192)    576         conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 5, 5, 192)    0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 5, 5, 224)    129024      activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 5, 5, 224)    672         conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 5, 5, 224)    0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 5, 5, 192)    399360      block8_7_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 5, 5, 256)    172032      activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 5, 5, 192)    576         conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 5, 5, 256)    768         conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 5, 5, 192)    0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 5, 5, 256)    0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_8_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_191[0][0]             \n",
            "                                                                 activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_8_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_8_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_8 (Lambda)               (None, 5, 5, 2080)   0           block8_7_ac[0][0]                \n",
            "                                                                 block8_8_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_8_ac (Activation)        (None, 5, 5, 2080)   0           block8_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 5, 5, 192)    399360      block8_8_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 5, 5, 192)    576         conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 5, 5, 192)    0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 5, 5, 224)    129024      activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 5, 5, 224)    672         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 5, 5, 224)    0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 5, 5, 192)    399360      block8_8_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 5, 5, 256)    172032      activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 5, 5, 192)    576         conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 5, 5, 256)    768         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 5, 5, 192)    0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 5, 5, 256)    0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_9_mixed (Concatenate)    (None, 5, 5, 448)    0           activation_195[0][0]             \n",
            "                                                                 activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_9_conv (Conv2D)          (None, 5, 5, 2080)   933920      block8_9_mixed[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_9 (Lambda)               (None, 5, 5, 2080)   0           block8_8_ac[0][0]                \n",
            "                                                                 block8_9_conv[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block8_9_ac (Activation)        (None, 5, 5, 2080)   0           block8_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 5, 5, 192)    399360      block8_9_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 5, 5, 192)    576         conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 5, 5, 192)    0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 5, 5, 224)    129024      activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 5, 5, 224)    672         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 5, 5, 224)    0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 5, 5, 192)    399360      block8_9_ac[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 5, 5, 256)    172032      activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 5, 5, 192)    576         conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 5, 5, 256)    768         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 5, 5, 192)    0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 5, 5, 256)    0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block8_10_mixed (Concatenate)   (None, 5, 5, 448)    0           activation_199[0][0]             \n",
            "                                                                 activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block8_10_conv (Conv2D)         (None, 5, 5, 2080)   933920      block8_10_mixed[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_10 (Lambda)              (None, 5, 5, 2080)   0           block8_9_ac[0][0]                \n",
            "                                                                 block8_10_conv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_7b (Conv2D)                (None, 5, 5, 1536)   3194880     block8_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_7b_bn (BatchNormalization) (None, 5, 5, 1536)   4608        conv_7b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_7b_ac (Activation)         (None, 5, 5, 1536)   0           conv_7b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 38400)        0           conv_7b_ac[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            115203      flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 54,451,939\n",
            "Trainable params: 115,203\n",
            "Non-trainable params: 54,336,736\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opAOu_xua4T2"
      },
      "source": [
        "\n",
        "\n",
        "# tell the model what cost and optimization method to use\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdFcytCPxPSv"
      },
      "source": [
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# model.compile(\n",
        "#   loss='categorical_crossentropy',\n",
        "#   optimizer=Adam(lr=0.0001),\n",
        "#   metrics=['accuracy']\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug5pkWQ8a4T3"
      },
      "source": [
        "\n",
        "\n",
        "# Use the Image Data Generator to import the images from the dataset\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_path = '/content/drive/MyDrive/Horizontal_bar_data/Training_Set'\n",
        "valid_path = '/content/drive/MyDrive/Horizontal_bar_data/Test_Set'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va1JNVxUa4T3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cad3b89-c214-498c-ff68-bdaa5d6fe4d4"
      },
      "source": [
        "# Make sure you provide the same target size as initialied for the image size\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/Horizontal_bar_data/Training_Set',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1224 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x50OlX0ua4T4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96cb3b06-a07a-4f2d-d742-7dd1eed8fcb1"
      },
      "source": [
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/Horizontal_bar_data/Test_Set',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 182 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mesHflbNyI4d"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"in_resnet_weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJBAzx7Ba4T4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef007e80-7f38-4af2-826e-f2c0ef1cb6fc"
      },
      "source": [
        "\n",
        "\n",
        "# fit the model\n",
        "# Run the cell. It will take some time to execute\n",
        "r = model.fit(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=500,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set),\n",
        "  callbacks=callbacks_list\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.8878 - accuracy: 0.6152\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.80220, saving model to in_resnet_weights-improvement-01-0.80.hdf5\n",
            "39/39 [==============================] - 323s 8s/step - loss: 1.8878 - accuracy: 0.6152 - val_loss: 0.6447 - val_accuracy: 0.8022\n",
            "Epoch 2/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.6618 - accuracy: 0.8170\n",
            "Epoch 00002: val_accuracy improved from 0.80220 to 0.83516, saving model to in_resnet_weights-improvement-02-0.84.hdf5\n",
            "39/39 [==============================] - 26s 659ms/step - loss: 0.6618 - accuracy: 0.8170 - val_loss: 0.4466 - val_accuracy: 0.8352\n",
            "Epoch 3/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.5373 - accuracy: 0.8415\n",
            "Epoch 00003: val_accuracy improved from 0.83516 to 0.85714, saving model to in_resnet_weights-improvement-03-0.86.hdf5\n",
            "39/39 [==============================] - 26s 660ms/step - loss: 0.5373 - accuracy: 0.8415 - val_loss: 0.5230 - val_accuracy: 0.8571\n",
            "Epoch 4/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.3804 - accuracy: 0.8775\n",
            "Epoch 00004: val_accuracy improved from 0.85714 to 0.92308, saving model to in_resnet_weights-improvement-04-0.92.hdf5\n",
            "39/39 [==============================] - 26s 660ms/step - loss: 0.3804 - accuracy: 0.8775 - val_loss: 0.2731 - val_accuracy: 0.9231\n",
            "Epoch 5/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.8987\n",
            "Epoch 00005: val_accuracy did not improve from 0.92308\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.2927 - accuracy: 0.8987 - val_loss: 0.2916 - val_accuracy: 0.9121\n",
            "Epoch 6/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.9134\n",
            "Epoch 00006: val_accuracy did not improve from 0.92308\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.2963 - accuracy: 0.9134 - val_loss: 0.3326 - val_accuracy: 0.9121\n",
            "Epoch 7/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.8578\n",
            "Epoch 00007: val_accuracy did not improve from 0.92308\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.5542 - accuracy: 0.8578 - val_loss: 0.2750 - val_accuracy: 0.9011\n",
            "Epoch 8/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9199\n",
            "Epoch 00008: val_accuracy did not improve from 0.92308\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.2454 - accuracy: 0.9199 - val_loss: 0.4406 - val_accuracy: 0.8956\n",
            "Epoch 9/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9518\n",
            "Epoch 00009: val_accuracy improved from 0.92308 to 0.93407, saving model to in_resnet_weights-improvement-09-0.93.hdf5\n",
            "39/39 [==============================] - 26s 656ms/step - loss: 0.1508 - accuracy: 0.9518 - val_loss: 0.2574 - val_accuracy: 0.9341\n",
            "Epoch 10/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9371\n",
            "Epoch 00010: val_accuracy did not improve from 0.93407\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.2409 - accuracy: 0.9371 - val_loss: 0.2553 - val_accuracy: 0.9341\n",
            "Epoch 11/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9485\n",
            "Epoch 00011: val_accuracy did not improve from 0.93407\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1573 - accuracy: 0.9485 - val_loss: 0.4214 - val_accuracy: 0.9066\n",
            "Epoch 12/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2280 - accuracy: 0.9371\n",
            "Epoch 00012: val_accuracy improved from 0.93407 to 0.95055, saving model to in_resnet_weights-improvement-12-0.95.hdf5\n",
            "39/39 [==============================] - 26s 658ms/step - loss: 0.2280 - accuracy: 0.9371 - val_loss: 0.2235 - val_accuracy: 0.9505\n",
            "Epoch 13/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.9412\n",
            "Epoch 00013: val_accuracy did not improve from 0.95055\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.2008 - accuracy: 0.9412 - val_loss: 0.5982 - val_accuracy: 0.8736\n",
            "Epoch 14/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.3717 - accuracy: 0.9093\n",
            "Epoch 00014: val_accuracy did not improve from 0.95055\n",
            "39/39 [==============================] - 25s 641ms/step - loss: 0.3717 - accuracy: 0.9093 - val_loss: 0.4458 - val_accuracy: 0.9176\n",
            "Epoch 15/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9436\n",
            "Epoch 00015: val_accuracy did not improve from 0.95055\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.1773 - accuracy: 0.9436 - val_loss: 0.3187 - val_accuracy: 0.9286\n",
            "Epoch 16/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.9077\n",
            "Epoch 00016: val_accuracy did not improve from 0.95055\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.3515 - accuracy: 0.9077 - val_loss: 0.7481 - val_accuracy: 0.8791\n",
            "Epoch 17/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9346\n",
            "Epoch 00017: val_accuracy did not improve from 0.95055\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.2471 - accuracy: 0.9346 - val_loss: 0.3509 - val_accuracy: 0.9341\n",
            "Epoch 18/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9665\n",
            "Epoch 00018: val_accuracy did not improve from 0.95055\n",
            "39/39 [==============================] - 24s 621ms/step - loss: 0.1022 - accuracy: 0.9665 - val_loss: 0.3321 - val_accuracy: 0.9396\n",
            "Epoch 19/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9649\n",
            "Epoch 00019: val_accuracy did not improve from 0.95055\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.1255 - accuracy: 0.9649 - val_loss: 0.4890 - val_accuracy: 0.9066\n",
            "Epoch 20/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9649\n",
            "Epoch 00020: val_accuracy improved from 0.95055 to 0.95604, saving model to in_resnet_weights-improvement-20-0.96.hdf5\n",
            "39/39 [==============================] - 26s 663ms/step - loss: 0.1272 - accuracy: 0.9649 - val_loss: 0.3127 - val_accuracy: 0.9560\n",
            "Epoch 21/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9632\n",
            "Epoch 00021: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 623ms/step - loss: 0.1088 - accuracy: 0.9632 - val_loss: 0.3208 - val_accuracy: 0.9341\n",
            "Epoch 22/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 0.9453\n",
            "Epoch 00022: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.2143 - accuracy: 0.9453 - val_loss: 0.3248 - val_accuracy: 0.9396\n",
            "Epoch 23/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9624\n",
            "Epoch 00023: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1632 - accuracy: 0.9624 - val_loss: 0.3632 - val_accuracy: 0.9341\n",
            "Epoch 24/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9779\n",
            "Epoch 00024: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0992 - accuracy: 0.9779 - val_loss: 0.5359 - val_accuracy: 0.9121\n",
            "Epoch 25/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9779\n",
            "Epoch 00025: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.0766 - accuracy: 0.9779 - val_loss: 0.8414 - val_accuracy: 0.8571\n",
            "Epoch 26/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9714\n",
            "Epoch 00026: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 628ms/step - loss: 0.0930 - accuracy: 0.9714 - val_loss: 0.3495 - val_accuracy: 0.9286\n",
            "Epoch 27/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9453\n",
            "Epoch 00027: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 624ms/step - loss: 0.2244 - accuracy: 0.9453 - val_loss: 0.5077 - val_accuracy: 0.9066\n",
            "Epoch 28/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9608\n",
            "Epoch 00028: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1362 - accuracy: 0.9608 - val_loss: 0.3098 - val_accuracy: 0.9286\n",
            "Epoch 29/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9804\n",
            "Epoch 00029: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.0679 - accuracy: 0.9804 - val_loss: 0.3799 - val_accuracy: 0.9176\n",
            "Epoch 30/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9698\n",
            "Epoch 00030: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.1247 - accuracy: 0.9698 - val_loss: 0.4056 - val_accuracy: 0.9341\n",
            "Epoch 31/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9714\n",
            "Epoch 00031: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 622ms/step - loss: 0.0762 - accuracy: 0.9714 - val_loss: 0.3223 - val_accuracy: 0.9451\n",
            "Epoch 32/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9722\n",
            "Epoch 00032: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 628ms/step - loss: 0.1121 - accuracy: 0.9722 - val_loss: 0.5646 - val_accuracy: 0.9066\n",
            "Epoch 33/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9681\n",
            "Epoch 00033: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.1141 - accuracy: 0.9681 - val_loss: 0.3891 - val_accuracy: 0.9231\n",
            "Epoch 34/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9845\n",
            "Epoch 00034: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0840 - accuracy: 0.9845 - val_loss: 0.3281 - val_accuracy: 0.9451\n",
            "Epoch 35/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9869\n",
            "Epoch 00035: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0620 - accuracy: 0.9869 - val_loss: 0.3399 - val_accuracy: 0.9560\n",
            "Epoch 36/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9804\n",
            "Epoch 00036: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 624ms/step - loss: 0.0688 - accuracy: 0.9804 - val_loss: 0.3550 - val_accuracy: 0.9560\n",
            "Epoch 37/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9812\n",
            "Epoch 00037: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0811 - accuracy: 0.9812 - val_loss: 0.3887 - val_accuracy: 0.9341\n",
            "Epoch 38/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9828\n",
            "Epoch 00038: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0641 - accuracy: 0.9828 - val_loss: 0.5668 - val_accuracy: 0.9121\n",
            "Epoch 39/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9559\n",
            "Epoch 00039: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1901 - accuracy: 0.9559 - val_loss: 0.7355 - val_accuracy: 0.8791\n",
            "Epoch 40/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9722\n",
            "Epoch 00040: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.1188 - accuracy: 0.9722 - val_loss: 0.4655 - val_accuracy: 0.9341\n",
            "Epoch 41/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9779\n",
            "Epoch 00041: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1058 - accuracy: 0.9779 - val_loss: 0.4513 - val_accuracy: 0.9341\n",
            "Epoch 42/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9788\n",
            "Epoch 00042: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0788 - accuracy: 0.9788 - val_loss: 0.3019 - val_accuracy: 0.9396\n",
            "Epoch 43/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9796\n",
            "Epoch 00043: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0798 - accuracy: 0.9796 - val_loss: 0.9230 - val_accuracy: 0.8626\n",
            "Epoch 44/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9747\n",
            "Epoch 00044: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1061 - accuracy: 0.9747 - val_loss: 0.2308 - val_accuracy: 0.9396\n",
            "Epoch 45/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9747\n",
            "Epoch 00045: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.1043 - accuracy: 0.9747 - val_loss: 0.1974 - val_accuracy: 0.9560\n",
            "Epoch 46/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.5195 - accuracy: 0.9167\n",
            "Epoch 00046: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.5195 - accuracy: 0.9167 - val_loss: 0.5813 - val_accuracy: 0.9286\n",
            "Epoch 47/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9657\n",
            "Epoch 00047: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.1554 - accuracy: 0.9657 - val_loss: 0.4296 - val_accuracy: 0.9396\n",
            "Epoch 48/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9812\n",
            "Epoch 00048: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1004 - accuracy: 0.9812 - val_loss: 0.8009 - val_accuracy: 0.8846\n",
            "Epoch 49/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1224 - accuracy: 0.9722\n",
            "Epoch 00049: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 646ms/step - loss: 0.1224 - accuracy: 0.9722 - val_loss: 0.4043 - val_accuracy: 0.9286\n",
            "Epoch 50/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2026 - accuracy: 0.9608\n",
            "Epoch 00050: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 624ms/step - loss: 0.2026 - accuracy: 0.9608 - val_loss: 0.4975 - val_accuracy: 0.9121\n",
            "Epoch 51/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9755\n",
            "Epoch 00051: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1717 - accuracy: 0.9755 - val_loss: 0.3907 - val_accuracy: 0.9560\n",
            "Epoch 52/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9730\n",
            "Epoch 00052: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.1257 - accuracy: 0.9730 - val_loss: 0.4961 - val_accuracy: 0.9451\n",
            "Epoch 53/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9739\n",
            "Epoch 00053: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1194 - accuracy: 0.9739 - val_loss: 0.6507 - val_accuracy: 0.9121\n",
            "Epoch 54/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9869\n",
            "Epoch 00054: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.0835 - accuracy: 0.9869 - val_loss: 0.2970 - val_accuracy: 0.9560\n",
            "Epoch 55/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9755\n",
            "Epoch 00055: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.1032 - accuracy: 0.9755 - val_loss: 0.4040 - val_accuracy: 0.9451\n",
            "Epoch 56/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.9714\n",
            "Epoch 00056: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 646ms/step - loss: 0.1460 - accuracy: 0.9714 - val_loss: 0.3951 - val_accuracy: 0.9341\n",
            "Epoch 57/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.9395\n",
            "Epoch 00057: val_accuracy did not improve from 0.95604\n",
            "39/39 [==============================] - 25s 648ms/step - loss: 0.3480 - accuracy: 0.9395 - val_loss: 0.3818 - val_accuracy: 0.9341\n",
            "Epoch 58/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9681\n",
            "Epoch 00058: val_accuracy improved from 0.95604 to 0.96154, saving model to in_resnet_weights-improvement-58-0.96.hdf5\n",
            "39/39 [==============================] - 26s 670ms/step - loss: 0.1337 - accuracy: 0.9681 - val_loss: 0.4071 - val_accuracy: 0.9615\n",
            "Epoch 59/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9828\n",
            "Epoch 00059: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1148 - accuracy: 0.9828 - val_loss: 0.3048 - val_accuracy: 0.9560\n",
            "Epoch 60/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9812\n",
            "Epoch 00060: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0742 - accuracy: 0.9812 - val_loss: 0.3287 - val_accuracy: 0.9505\n",
            "Epoch 61/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9837\n",
            "Epoch 00061: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.1119 - accuracy: 0.9837 - val_loss: 0.4153 - val_accuracy: 0.9396\n",
            "Epoch 62/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9722\n",
            "Epoch 00062: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1220 - accuracy: 0.9722 - val_loss: 0.7223 - val_accuracy: 0.8956\n",
            "Epoch 63/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9779\n",
            "Epoch 00063: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0790 - accuracy: 0.9779 - val_loss: 0.7878 - val_accuracy: 0.8791\n",
            "Epoch 64/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.9542\n",
            "Epoch 00064: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.2551 - accuracy: 0.9542 - val_loss: 0.9022 - val_accuracy: 0.8956\n",
            "Epoch 65/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9690\n",
            "Epoch 00065: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.1336 - accuracy: 0.9690 - val_loss: 0.9463 - val_accuracy: 0.8956\n",
            "Epoch 66/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9779\n",
            "Epoch 00066: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 623ms/step - loss: 0.1176 - accuracy: 0.9779 - val_loss: 0.7943 - val_accuracy: 0.9286\n",
            "Epoch 67/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9845\n",
            "Epoch 00067: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1362 - accuracy: 0.9845 - val_loss: 0.5130 - val_accuracy: 0.9176\n",
            "Epoch 68/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9894\n",
            "Epoch 00068: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.0786 - accuracy: 0.9894 - val_loss: 0.4185 - val_accuracy: 0.9451\n",
            "Epoch 69/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9935\n",
            "Epoch 00069: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 620ms/step - loss: 0.0397 - accuracy: 0.9935 - val_loss: 0.4777 - val_accuracy: 0.9396\n",
            "Epoch 70/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9902\n",
            "Epoch 00070: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0445 - accuracy: 0.9902 - val_loss: 0.5893 - val_accuracy: 0.9341\n",
            "Epoch 71/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9935\n",
            "Epoch 00071: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.0421 - accuracy: 0.9935 - val_loss: 0.3868 - val_accuracy: 0.9505\n",
            "Epoch 72/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9804\n",
            "Epoch 00072: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 25s 638ms/step - loss: 0.0716 - accuracy: 0.9804 - val_loss: 0.9028 - val_accuracy: 0.9066\n",
            "Epoch 73/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9722\n",
            "Epoch 00073: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 624ms/step - loss: 0.1127 - accuracy: 0.9722 - val_loss: 0.5027 - val_accuracy: 0.9451\n",
            "Epoch 74/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9763\n",
            "Epoch 00074: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 624ms/step - loss: 0.1464 - accuracy: 0.9763 - val_loss: 0.4857 - val_accuracy: 0.9341\n",
            "Epoch 75/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9739\n",
            "Epoch 00075: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.1388 - accuracy: 0.9739 - val_loss: 0.6712 - val_accuracy: 0.9286\n",
            "Epoch 76/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9771\n",
            "Epoch 00076: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1247 - accuracy: 0.9771 - val_loss: 0.5204 - val_accuracy: 0.9176\n",
            "Epoch 77/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9804\n",
            "Epoch 00077: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0910 - accuracy: 0.9804 - val_loss: 0.5729 - val_accuracy: 0.9231\n",
            "Epoch 78/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9788\n",
            "Epoch 00078: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.1601 - accuracy: 0.9788 - val_loss: 1.1670 - val_accuracy: 0.8846\n",
            "Epoch 79/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9779\n",
            "Epoch 00079: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.1070 - accuracy: 0.9779 - val_loss: 0.4894 - val_accuracy: 0.9615\n",
            "Epoch 80/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9837\n",
            "Epoch 00080: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 623ms/step - loss: 0.0911 - accuracy: 0.9837 - val_loss: 0.3892 - val_accuracy: 0.9396\n",
            "Epoch 81/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9886\n",
            "Epoch 00081: val_accuracy did not improve from 0.96154\n",
            "39/39 [==============================] - 24s 623ms/step - loss: 0.0632 - accuracy: 0.9886 - val_loss: 0.4051 - val_accuracy: 0.9505\n",
            "Epoch 82/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9779\n",
            "Epoch 00082: val_accuracy improved from 0.96154 to 0.96703, saving model to in_resnet_weights-improvement-82-0.97.hdf5\n",
            "39/39 [==============================] - 26s 656ms/step - loss: 0.0946 - accuracy: 0.9779 - val_loss: 0.3982 - val_accuracy: 0.9670\n",
            "Epoch 83/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9886\n",
            "Epoch 00083: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0603 - accuracy: 0.9886 - val_loss: 0.4390 - val_accuracy: 0.9670\n",
            "Epoch 84/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9779\n",
            "Epoch 00084: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.1029 - accuracy: 0.9779 - val_loss: 0.4370 - val_accuracy: 0.9615\n",
            "Epoch 85/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9894\n",
            "Epoch 00085: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.0365 - accuracy: 0.9894 - val_loss: 0.4908 - val_accuracy: 0.9451\n",
            "Epoch 86/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 0.9812\n",
            "Epoch 00086: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.1248 - accuracy: 0.9812 - val_loss: 0.7005 - val_accuracy: 0.9121\n",
            "Epoch 87/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9845\n",
            "Epoch 00087: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0550 - accuracy: 0.9845 - val_loss: 0.3818 - val_accuracy: 0.9670\n",
            "Epoch 88/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9869\n",
            "Epoch 00088: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.1184 - accuracy: 0.9869 - val_loss: 0.3477 - val_accuracy: 0.9615\n",
            "Epoch 89/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.9698\n",
            "Epoch 00089: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.1911 - accuracy: 0.9698 - val_loss: 0.6211 - val_accuracy: 0.9341\n",
            "Epoch 90/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.9420\n",
            "Epoch 00090: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 623ms/step - loss: 0.4704 - accuracy: 0.9420 - val_loss: 0.5253 - val_accuracy: 0.9396\n",
            "Epoch 91/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.3804 - accuracy: 0.9420\n",
            "Epoch 00091: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.3804 - accuracy: 0.9420 - val_loss: 1.2806 - val_accuracy: 0.8571\n",
            "Epoch 92/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9812\n",
            "Epoch 00092: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 624ms/step - loss: 0.1965 - accuracy: 0.9812 - val_loss: 0.5950 - val_accuracy: 0.9560\n",
            "Epoch 93/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9877\n",
            "Epoch 00093: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.0630 - accuracy: 0.9877 - val_loss: 0.5849 - val_accuracy: 0.9396\n",
            "Epoch 94/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9837\n",
            "Epoch 00094: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.1044 - accuracy: 0.9837 - val_loss: 0.4954 - val_accuracy: 0.9560\n",
            "Epoch 95/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9755\n",
            "Epoch 00095: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.1462 - accuracy: 0.9755 - val_loss: 1.0132 - val_accuracy: 0.8956\n",
            "Epoch 96/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9845\n",
            "Epoch 00096: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0929 - accuracy: 0.9845 - val_loss: 0.7538 - val_accuracy: 0.9341\n",
            "Epoch 97/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9747\n",
            "Epoch 00097: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.1419 - accuracy: 0.9747 - val_loss: 1.0372 - val_accuracy: 0.9066\n",
            "Epoch 98/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9681\n",
            "Epoch 00098: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.2513 - accuracy: 0.9681 - val_loss: 0.7143 - val_accuracy: 0.9451\n",
            "Epoch 99/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.9526\n",
            "Epoch 00099: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 623ms/step - loss: 0.2273 - accuracy: 0.9526 - val_loss: 0.8655 - val_accuracy: 0.9286\n",
            "Epoch 100/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9657\n",
            "Epoch 00100: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.2872 - accuracy: 0.9657 - val_loss: 1.1023 - val_accuracy: 0.9066\n",
            "Epoch 101/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1653 - accuracy: 0.9845\n",
            "Epoch 00101: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.1653 - accuracy: 0.9845 - val_loss: 0.7598 - val_accuracy: 0.9451\n",
            "Epoch 102/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9886\n",
            "Epoch 00102: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0720 - accuracy: 0.9886 - val_loss: 0.6508 - val_accuracy: 0.9451\n",
            "Epoch 103/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9894\n",
            "Epoch 00103: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.0638 - accuracy: 0.9894 - val_loss: 0.8103 - val_accuracy: 0.9451\n",
            "Epoch 104/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9828\n",
            "Epoch 00104: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 638ms/step - loss: 0.1562 - accuracy: 0.9828 - val_loss: 0.7902 - val_accuracy: 0.9451\n",
            "Epoch 105/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9869\n",
            "Epoch 00105: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.0633 - accuracy: 0.9869 - val_loss: 0.4974 - val_accuracy: 0.9615\n",
            "Epoch 106/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9804\n",
            "Epoch 00106: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1180 - accuracy: 0.9804 - val_loss: 0.6050 - val_accuracy: 0.9396\n",
            "Epoch 107/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9763\n",
            "Epoch 00107: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.1767 - accuracy: 0.9763 - val_loss: 0.7043 - val_accuracy: 0.9341\n",
            "Epoch 108/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9894\n",
            "Epoch 00108: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.0990 - accuracy: 0.9894 - val_loss: 0.9900 - val_accuracy: 0.9121\n",
            "Epoch 109/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9869\n",
            "Epoch 00109: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 640ms/step - loss: 0.0915 - accuracy: 0.9869 - val_loss: 0.5219 - val_accuracy: 0.9341\n",
            "Epoch 110/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9779\n",
            "Epoch 00110: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.1729 - accuracy: 0.9779 - val_loss: 0.9603 - val_accuracy: 0.9176\n",
            "Epoch 111/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9730\n",
            "Epoch 00111: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.1810 - accuracy: 0.9730 - val_loss: 0.7406 - val_accuracy: 0.9341\n",
            "Epoch 112/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9894\n",
            "Epoch 00112: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.0640 - accuracy: 0.9894 - val_loss: 0.8588 - val_accuracy: 0.9286\n",
            "Epoch 113/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1791 - accuracy: 0.9739\n",
            "Epoch 00113: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1791 - accuracy: 0.9739 - val_loss: 0.3944 - val_accuracy: 0.9615\n",
            "Epoch 114/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9837\n",
            "Epoch 00114: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.1085 - accuracy: 0.9837 - val_loss: 0.9445 - val_accuracy: 0.9066\n",
            "Epoch 115/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9861\n",
            "Epoch 00115: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.1433 - accuracy: 0.9861 - val_loss: 0.6842 - val_accuracy: 0.9505\n",
            "Epoch 116/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9869\n",
            "Epoch 00116: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.0358 - accuracy: 0.9869 - val_loss: 0.6912 - val_accuracy: 0.9396\n",
            "Epoch 117/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9869\n",
            "Epoch 00117: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0838 - accuracy: 0.9869 - val_loss: 0.6108 - val_accuracy: 0.9396\n",
            "Epoch 118/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9935\n",
            "Epoch 00118: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0228 - accuracy: 0.9935 - val_loss: 1.2269 - val_accuracy: 0.9066\n",
            "Epoch 119/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9902\n",
            "Epoch 00119: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.0589 - accuracy: 0.9902 - val_loss: 0.6305 - val_accuracy: 0.9341\n",
            "Epoch 120/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9812\n",
            "Epoch 00120: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1627 - accuracy: 0.9812 - val_loss: 0.6547 - val_accuracy: 0.9560\n",
            "Epoch 121/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9877\n",
            "Epoch 00121: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0750 - accuracy: 0.9877 - val_loss: 0.7673 - val_accuracy: 0.9451\n",
            "Epoch 122/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9910\n",
            "Epoch 00122: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0964 - accuracy: 0.9910 - val_loss: 0.5771 - val_accuracy: 0.9341\n",
            "Epoch 123/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9943\n",
            "Epoch 00123: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 628ms/step - loss: 0.0441 - accuracy: 0.9943 - val_loss: 0.6141 - val_accuracy: 0.9451\n",
            "Epoch 124/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9918\n",
            "Epoch 00124: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.0757 - accuracy: 0.9918 - val_loss: 0.8731 - val_accuracy: 0.9396\n",
            "Epoch 125/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9926\n",
            "Epoch 00125: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.0181 - accuracy: 0.9926 - val_loss: 0.7130 - val_accuracy: 0.9341\n",
            "Epoch 126/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9820\n",
            "Epoch 00126: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.1292 - accuracy: 0.9820 - val_loss: 0.4905 - val_accuracy: 0.9560\n",
            "Epoch 127/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9820\n",
            "Epoch 00127: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.1446 - accuracy: 0.9820 - val_loss: 0.5498 - val_accuracy: 0.9560\n",
            "Epoch 128/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9894\n",
            "Epoch 00128: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0959 - accuracy: 0.9894 - val_loss: 0.5618 - val_accuracy: 0.9451\n",
            "Epoch 129/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9910\n",
            "Epoch 00129: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0555 - accuracy: 0.9910 - val_loss: 0.3582 - val_accuracy: 0.9670\n",
            "Epoch 130/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 0.9747\n",
            "Epoch 00130: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.1585 - accuracy: 0.9747 - val_loss: 0.4103 - val_accuracy: 0.9670\n",
            "Epoch 131/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.9469\n",
            "Epoch 00131: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.4964 - accuracy: 0.9469 - val_loss: 1.4222 - val_accuracy: 0.8956\n",
            "Epoch 132/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.9755\n",
            "Epoch 00132: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.2584 - accuracy: 0.9755 - val_loss: 0.5697 - val_accuracy: 0.9451\n",
            "Epoch 133/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9853\n",
            "Epoch 00133: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0983 - accuracy: 0.9853 - val_loss: 0.5942 - val_accuracy: 0.9615\n",
            "Epoch 134/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9910\n",
            "Epoch 00134: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 638ms/step - loss: 0.0602 - accuracy: 0.9910 - val_loss: 0.6641 - val_accuracy: 0.9341\n",
            "Epoch 135/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9853\n",
            "Epoch 00135: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.0899 - accuracy: 0.9853 - val_loss: 0.6848 - val_accuracy: 0.9505\n",
            "Epoch 136/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9967\n",
            "Epoch 00136: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 645ms/step - loss: 0.0332 - accuracy: 0.9967 - val_loss: 0.7017 - val_accuracy: 0.9505\n",
            "Epoch 137/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9951\n",
            "Epoch 00137: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 624ms/step - loss: 0.0823 - accuracy: 0.9951 - val_loss: 1.0542 - val_accuracy: 0.9066\n",
            "Epoch 138/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9869\n",
            "Epoch 00138: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0962 - accuracy: 0.9869 - val_loss: 0.5197 - val_accuracy: 0.9615\n",
            "Epoch 139/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9886\n",
            "Epoch 00139: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0637 - accuracy: 0.9886 - val_loss: 0.5193 - val_accuracy: 0.9615\n",
            "Epoch 140/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9894\n",
            "Epoch 00140: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0853 - accuracy: 0.9894 - val_loss: 0.8231 - val_accuracy: 0.9560\n",
            "Epoch 141/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9894\n",
            "Epoch 00141: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.0981 - accuracy: 0.9894 - val_loss: 1.0137 - val_accuracy: 0.9066\n",
            "Epoch 142/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9935\n",
            "Epoch 00142: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0503 - accuracy: 0.9935 - val_loss: 1.0578 - val_accuracy: 0.9231\n",
            "Epoch 143/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.9771\n",
            "Epoch 00143: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.2097 - accuracy: 0.9771 - val_loss: 0.7909 - val_accuracy: 0.9341\n",
            "Epoch 144/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9820\n",
            "Epoch 00144: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.1087 - accuracy: 0.9820 - val_loss: 0.6320 - val_accuracy: 0.9396\n",
            "Epoch 145/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9959\n",
            "Epoch 00145: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0235 - accuracy: 0.9959 - val_loss: 0.7057 - val_accuracy: 0.9231\n",
            "Epoch 146/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9869\n",
            "Epoch 00146: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.0820 - accuracy: 0.9869 - val_loss: 0.5669 - val_accuracy: 0.9670\n",
            "Epoch 147/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9820\n",
            "Epoch 00147: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 625ms/step - loss: 0.1714 - accuracy: 0.9820 - val_loss: 0.6131 - val_accuracy: 0.9560\n",
            "Epoch 148/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9828\n",
            "Epoch 00148: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1573 - accuracy: 0.9828 - val_loss: 0.5240 - val_accuracy: 0.9560\n",
            "Epoch 149/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9853\n",
            "Epoch 00149: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1272 - accuracy: 0.9853 - val_loss: 1.3720 - val_accuracy: 0.9066\n",
            "Epoch 150/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9886\n",
            "Epoch 00150: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 640ms/step - loss: 0.1176 - accuracy: 0.9886 - val_loss: 0.8521 - val_accuracy: 0.9341\n",
            "Epoch 151/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9739\n",
            "Epoch 00151: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.1624 - accuracy: 0.9739 - val_loss: 0.4067 - val_accuracy: 0.9560\n",
            "Epoch 152/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9886\n",
            "Epoch 00152: val_accuracy did not improve from 0.96703\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.0563 - accuracy: 0.9886 - val_loss: 0.7177 - val_accuracy: 0.9176\n",
            "Epoch 153/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9894\n",
            "Epoch 00153: val_accuracy improved from 0.96703 to 0.97802, saving model to in_resnet_weights-improvement-153-0.98.hdf5\n",
            "39/39 [==============================] - 26s 674ms/step - loss: 0.0835 - accuracy: 0.9894 - val_loss: 0.4357 - val_accuracy: 0.9780\n",
            "Epoch 154/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9812\n",
            "Epoch 00154: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1521 - accuracy: 0.9812 - val_loss: 1.2451 - val_accuracy: 0.8956\n",
            "Epoch 155/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9877\n",
            "Epoch 00155: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0699 - accuracy: 0.9877 - val_loss: 0.4342 - val_accuracy: 0.9670\n",
            "Epoch 156/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9967\n",
            "Epoch 00156: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.0606 - accuracy: 0.9967 - val_loss: 0.4443 - val_accuracy: 0.9670\n",
            "Epoch 157/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9935\n",
            "Epoch 00157: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0759 - accuracy: 0.9935 - val_loss: 0.5692 - val_accuracy: 0.9505\n",
            "Epoch 158/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9910\n",
            "Epoch 00158: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.0470 - accuracy: 0.9910 - val_loss: 0.6148 - val_accuracy: 0.9560\n",
            "Epoch 159/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9910\n",
            "Epoch 00159: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.0589 - accuracy: 0.9910 - val_loss: 1.2100 - val_accuracy: 0.9176\n",
            "Epoch 160/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9943\n",
            "Epoch 00160: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0363 - accuracy: 0.9943 - val_loss: 0.6756 - val_accuracy: 0.9560\n",
            "Epoch 161/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.9714\n",
            "Epoch 00161: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.2683 - accuracy: 0.9714 - val_loss: 0.8341 - val_accuracy: 0.9341\n",
            "Epoch 162/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9673\n",
            "Epoch 00162: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.2421 - accuracy: 0.9673 - val_loss: 1.9030 - val_accuracy: 0.8791\n",
            "Epoch 163/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9869\n",
            "Epoch 00163: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 624ms/step - loss: 0.0653 - accuracy: 0.9869 - val_loss: 1.8514 - val_accuracy: 0.8791\n",
            "Epoch 164/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9877\n",
            "Epoch 00164: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.1447 - accuracy: 0.9877 - val_loss: 0.7579 - val_accuracy: 0.9560\n",
            "Epoch 165/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9886\n",
            "Epoch 00165: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.1117 - accuracy: 0.9886 - val_loss: 0.7091 - val_accuracy: 0.9560\n",
            "Epoch 166/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.9853\n",
            "Epoch 00166: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1327 - accuracy: 0.9853 - val_loss: 0.6240 - val_accuracy: 0.9670\n",
            "Epoch 167/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9861\n",
            "Epoch 00167: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.1068 - accuracy: 0.9861 - val_loss: 0.6937 - val_accuracy: 0.9560\n",
            "Epoch 168/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9869\n",
            "Epoch 00168: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.1092 - accuracy: 0.9869 - val_loss: 0.6651 - val_accuracy: 0.9670\n",
            "Epoch 169/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9902\n",
            "Epoch 00169: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0628 - accuracy: 0.9902 - val_loss: 0.6539 - val_accuracy: 0.9670\n",
            "Epoch 170/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9935\n",
            "Epoch 00170: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0349 - accuracy: 0.9935 - val_loss: 0.5711 - val_accuracy: 0.9725\n",
            "Epoch 171/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9902\n",
            "Epoch 00171: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 641ms/step - loss: 0.0893 - accuracy: 0.9902 - val_loss: 0.5934 - val_accuracy: 0.9725\n",
            "Epoch 172/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9951\n",
            "Epoch 00172: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.0464 - accuracy: 0.9951 - val_loss: 0.6529 - val_accuracy: 0.9615\n",
            "Epoch 173/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9959\n",
            "Epoch 00173: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0839 - accuracy: 0.9959 - val_loss: 0.5732 - val_accuracy: 0.9670\n",
            "Epoch 174/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9869\n",
            "Epoch 00174: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1175 - accuracy: 0.9869 - val_loss: 0.8143 - val_accuracy: 0.9505\n",
            "Epoch 175/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9902\n",
            "Epoch 00175: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 628ms/step - loss: 0.0738 - accuracy: 0.9902 - val_loss: 0.8277 - val_accuracy: 0.9451\n",
            "Epoch 176/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9861\n",
            "Epoch 00176: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1229 - accuracy: 0.9861 - val_loss: 0.4571 - val_accuracy: 0.9780\n",
            "Epoch 177/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9877\n",
            "Epoch 00177: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.1335 - accuracy: 0.9877 - val_loss: 0.5123 - val_accuracy: 0.9560\n",
            "Epoch 178/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9853\n",
            "Epoch 00178: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1314 - accuracy: 0.9853 - val_loss: 0.6827 - val_accuracy: 0.9505\n",
            "Epoch 179/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9935\n",
            "Epoch 00179: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0469 - accuracy: 0.9935 - val_loss: 0.9899 - val_accuracy: 0.9451\n",
            "Epoch 180/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9935\n",
            "Epoch 00180: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0654 - accuracy: 0.9935 - val_loss: 0.6943 - val_accuracy: 0.9560\n",
            "Epoch 181/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9910\n",
            "Epoch 00181: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0536 - accuracy: 0.9910 - val_loss: 1.0048 - val_accuracy: 0.9286\n",
            "Epoch 182/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9886\n",
            "Epoch 00182: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0729 - accuracy: 0.9886 - val_loss: 0.8257 - val_accuracy: 0.9505\n",
            "Epoch 183/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9796\n",
            "Epoch 00183: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.1688 - accuracy: 0.9796 - val_loss: 0.6671 - val_accuracy: 0.9615\n",
            "Epoch 184/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9845\n",
            "Epoch 00184: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.0847 - accuracy: 0.9845 - val_loss: 0.7345 - val_accuracy: 0.9505\n",
            "Epoch 185/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9845\n",
            "Epoch 00185: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.1293 - accuracy: 0.9845 - val_loss: 0.7367 - val_accuracy: 0.9505\n",
            "Epoch 186/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9837\n",
            "Epoch 00186: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1719 - accuracy: 0.9837 - val_loss: 0.7199 - val_accuracy: 0.9451\n",
            "Epoch 187/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9894\n",
            "Epoch 00187: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1022 - accuracy: 0.9894 - val_loss: 0.6068 - val_accuracy: 0.9670\n",
            "Epoch 188/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9935\n",
            "Epoch 00188: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0448 - accuracy: 0.9935 - val_loss: 1.8735 - val_accuracy: 0.8901\n",
            "Epoch 189/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9926\n",
            "Epoch 00189: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 626ms/step - loss: 0.0625 - accuracy: 0.9926 - val_loss: 1.3509 - val_accuracy: 0.9066\n",
            "Epoch 190/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9935\n",
            "Epoch 00190: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0544 - accuracy: 0.9935 - val_loss: 0.5788 - val_accuracy: 0.9560\n",
            "Epoch 191/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9820\n",
            "Epoch 00191: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.1787 - accuracy: 0.9820 - val_loss: 0.6933 - val_accuracy: 0.9615\n",
            "Epoch 192/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2170 - accuracy: 0.9747\n",
            "Epoch 00192: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.2170 - accuracy: 0.9747 - val_loss: 0.5243 - val_accuracy: 0.9615\n",
            "Epoch 193/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1964 - accuracy: 0.9788\n",
            "Epoch 00193: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.1964 - accuracy: 0.9788 - val_loss: 0.6887 - val_accuracy: 0.9560\n",
            "Epoch 194/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9869\n",
            "Epoch 00194: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.0713 - accuracy: 0.9869 - val_loss: 0.6377 - val_accuracy: 0.9615\n",
            "Epoch 195/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9951\n",
            "Epoch 00195: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0373 - accuracy: 0.9951 - val_loss: 0.6412 - val_accuracy: 0.9670\n",
            "Epoch 196/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9951\n",
            "Epoch 00196: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0627 - accuracy: 0.9951 - val_loss: 0.6686 - val_accuracy: 0.9615\n",
            "Epoch 197/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9951\n",
            "Epoch 00197: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0512 - accuracy: 0.9951 - val_loss: 0.6568 - val_accuracy: 0.9560\n",
            "Epoch 198/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9828\n",
            "Epoch 00198: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.1614 - accuracy: 0.9828 - val_loss: 1.2751 - val_accuracy: 0.9341\n",
            "Epoch 199/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9845\n",
            "Epoch 00199: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.1285 - accuracy: 0.9845 - val_loss: 0.6389 - val_accuracy: 0.9560\n",
            "Epoch 200/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9877\n",
            "Epoch 00200: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.1474 - accuracy: 0.9877 - val_loss: 0.4725 - val_accuracy: 0.9670\n",
            "Epoch 201/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9869\n",
            "Epoch 00201: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 628ms/step - loss: 0.1573 - accuracy: 0.9869 - val_loss: 0.8290 - val_accuracy: 0.9396\n",
            "Epoch 202/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9828\n",
            "Epoch 00202: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 24s 627ms/step - loss: 0.1479 - accuracy: 0.9828 - val_loss: 0.9285 - val_accuracy: 0.9286\n",
            "Epoch 203/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9943\n",
            "Epoch 00203: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.0627 - accuracy: 0.9943 - val_loss: 0.3664 - val_accuracy: 0.9615\n",
            "Epoch 204/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9926\n",
            "Epoch 00204: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0413 - accuracy: 0.9926 - val_loss: 0.5609 - val_accuracy: 0.9615\n",
            "Epoch 205/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9877\n",
            "Epoch 00205: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.1022 - accuracy: 0.9877 - val_loss: 0.6344 - val_accuracy: 0.9615\n",
            "Epoch 206/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9853\n",
            "Epoch 00206: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1005 - accuracy: 0.9853 - val_loss: 0.4320 - val_accuracy: 0.9725\n",
            "Epoch 207/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9935\n",
            "Epoch 00207: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0605 - accuracy: 0.9935 - val_loss: 0.7082 - val_accuracy: 0.9505\n",
            "Epoch 208/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9943\n",
            "Epoch 00208: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0716 - accuracy: 0.9943 - val_loss: 0.6029 - val_accuracy: 0.9615\n",
            "Epoch 209/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9894\n",
            "Epoch 00209: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0615 - accuracy: 0.9894 - val_loss: 1.4311 - val_accuracy: 0.9231\n",
            "Epoch 210/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9902\n",
            "Epoch 00210: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0632 - accuracy: 0.9902 - val_loss: 0.6675 - val_accuracy: 0.9560\n",
            "Epoch 211/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9886\n",
            "Epoch 00211: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0797 - accuracy: 0.9886 - val_loss: 0.9093 - val_accuracy: 0.9451\n",
            "Epoch 212/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9853\n",
            "Epoch 00212: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.2070 - accuracy: 0.9853 - val_loss: 0.6638 - val_accuracy: 0.9505\n",
            "Epoch 213/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9861\n",
            "Epoch 00213: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0899 - accuracy: 0.9861 - val_loss: 0.9756 - val_accuracy: 0.9505\n",
            "Epoch 214/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9877\n",
            "Epoch 00214: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.1631 - accuracy: 0.9877 - val_loss: 0.7497 - val_accuracy: 0.9505\n",
            "Epoch 215/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9837\n",
            "Epoch 00215: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.1830 - accuracy: 0.9837 - val_loss: 0.7900 - val_accuracy: 0.9451\n",
            "Epoch 216/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9820\n",
            "Epoch 00216: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.1181 - accuracy: 0.9820 - val_loss: 0.5324 - val_accuracy: 0.9725\n",
            "Epoch 217/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9943\n",
            "Epoch 00217: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1004 - accuracy: 0.9943 - val_loss: 3.7952 - val_accuracy: 0.8352\n",
            "Epoch 218/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9690\n",
            "Epoch 00218: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 646ms/step - loss: 0.2414 - accuracy: 0.9690 - val_loss: 0.7840 - val_accuracy: 0.9560\n",
            "Epoch 219/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9943\n",
            "Epoch 00219: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0413 - accuracy: 0.9943 - val_loss: 1.0510 - val_accuracy: 0.9341\n",
            "Epoch 220/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9877\n",
            "Epoch 00220: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.1094 - accuracy: 0.9877 - val_loss: 1.0935 - val_accuracy: 0.9396\n",
            "Epoch 221/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9935\n",
            "Epoch 00221: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.0469 - accuracy: 0.9935 - val_loss: 0.5650 - val_accuracy: 0.9615\n",
            "Epoch 222/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9918\n",
            "Epoch 00222: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0650 - accuracy: 0.9918 - val_loss: 1.2235 - val_accuracy: 0.9396\n",
            "Epoch 223/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9845\n",
            "Epoch 00223: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 628ms/step - loss: 0.1268 - accuracy: 0.9845 - val_loss: 0.5890 - val_accuracy: 0.9670\n",
            "Epoch 224/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9967\n",
            "Epoch 00224: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.0188 - accuracy: 0.9967 - val_loss: 0.4661 - val_accuracy: 0.9725\n",
            "Epoch 225/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9951\n",
            "Epoch 00225: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.0586 - accuracy: 0.9951 - val_loss: 0.6586 - val_accuracy: 0.9560\n",
            "Epoch 226/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9935\n",
            "Epoch 00226: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.0667 - accuracy: 0.9935 - val_loss: 0.5524 - val_accuracy: 0.9725\n",
            "Epoch 227/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9951\n",
            "Epoch 00227: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0597 - accuracy: 0.9951 - val_loss: 0.5803 - val_accuracy: 0.9615\n",
            "Epoch 228/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9886\n",
            "Epoch 00228: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.1039 - accuracy: 0.9886 - val_loss: 0.9426 - val_accuracy: 0.9286\n",
            "Epoch 229/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9869\n",
            "Epoch 00229: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0801 - accuracy: 0.9869 - val_loss: 2.0823 - val_accuracy: 0.8736\n",
            "Epoch 230/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9894\n",
            "Epoch 00230: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0563 - accuracy: 0.9894 - val_loss: 0.7607 - val_accuracy: 0.9396\n",
            "Epoch 231/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9910\n",
            "Epoch 00231: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.0663 - accuracy: 0.9910 - val_loss: 1.0080 - val_accuracy: 0.9231\n",
            "Epoch 232/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9984\n",
            "Epoch 00232: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0092 - accuracy: 0.9984 - val_loss: 0.9498 - val_accuracy: 0.9396\n",
            "Epoch 233/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9984\n",
            "Epoch 00233: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0168 - accuracy: 0.9984 - val_loss: 1.0368 - val_accuracy: 0.9396\n",
            "Epoch 234/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9967\n",
            "Epoch 00234: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0081 - accuracy: 0.9967 - val_loss: 0.7049 - val_accuracy: 0.9505\n",
            "Epoch 235/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9763\n",
            "Epoch 00235: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.1931 - accuracy: 0.9763 - val_loss: 1.3398 - val_accuracy: 0.9066\n",
            "Epoch 236/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9812\n",
            "Epoch 00236: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.2409 - accuracy: 0.9812 - val_loss: 0.8370 - val_accuracy: 0.9560\n",
            "Epoch 237/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9918\n",
            "Epoch 00237: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.1223 - accuracy: 0.9918 - val_loss: 0.6293 - val_accuracy: 0.9451\n",
            "Epoch 238/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.9812\n",
            "Epoch 00238: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1384 - accuracy: 0.9812 - val_loss: 0.8519 - val_accuracy: 0.9396\n",
            "Epoch 239/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.9796\n",
            "Epoch 00239: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.1878 - accuracy: 0.9796 - val_loss: 1.4638 - val_accuracy: 0.9231\n",
            "Epoch 240/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9926\n",
            "Epoch 00240: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.0457 - accuracy: 0.9926 - val_loss: 0.9682 - val_accuracy: 0.9451\n",
            "Epoch 241/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9959\n",
            "Epoch 00241: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0809 - accuracy: 0.9959 - val_loss: 0.8770 - val_accuracy: 0.9505\n",
            "Epoch 242/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9951\n",
            "Epoch 00242: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 630ms/step - loss: 0.0438 - accuracy: 0.9951 - val_loss: 1.4592 - val_accuracy: 0.9231\n",
            "Epoch 243/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9869\n",
            "Epoch 00243: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.1474 - accuracy: 0.9869 - val_loss: 0.9518 - val_accuracy: 0.9231\n",
            "Epoch 244/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9910\n",
            "Epoch 00244: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.1184 - accuracy: 0.9910 - val_loss: 0.6778 - val_accuracy: 0.9670\n",
            "Epoch 245/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9943\n",
            "Epoch 00245: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.0407 - accuracy: 0.9943 - val_loss: 0.8632 - val_accuracy: 0.9670\n",
            "Epoch 246/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9886\n",
            "Epoch 00246: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0748 - accuracy: 0.9886 - val_loss: 0.8363 - val_accuracy: 0.9451\n",
            "Epoch 247/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9926\n",
            "Epoch 00247: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0492 - accuracy: 0.9926 - val_loss: 0.8822 - val_accuracy: 0.9505\n",
            "Epoch 248/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9910\n",
            "Epoch 00248: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.0723 - accuracy: 0.9910 - val_loss: 1.1808 - val_accuracy: 0.9396\n",
            "Epoch 249/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9869\n",
            "Epoch 00249: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.0980 - accuracy: 0.9869 - val_loss: 0.9348 - val_accuracy: 0.9615\n",
            "Epoch 250/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.9861\n",
            "Epoch 00250: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 642ms/step - loss: 0.1655 - accuracy: 0.9861 - val_loss: 2.1251 - val_accuracy: 0.8846\n",
            "Epoch 251/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9951\n",
            "Epoch 00251: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0869 - accuracy: 0.9951 - val_loss: 0.6267 - val_accuracy: 0.9615\n",
            "Epoch 252/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9943\n",
            "Epoch 00252: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.0782 - accuracy: 0.9943 - val_loss: 0.5961 - val_accuracy: 0.9670\n",
            "Epoch 253/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9967\n",
            "Epoch 00253: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.0542 - accuracy: 0.9967 - val_loss: 0.6598 - val_accuracy: 0.9670\n",
            "Epoch 254/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9894\n",
            "Epoch 00254: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 631ms/step - loss: 0.1359 - accuracy: 0.9894 - val_loss: 1.5056 - val_accuracy: 0.9231\n",
            "Epoch 255/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9828\n",
            "Epoch 00255: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 641ms/step - loss: 0.1401 - accuracy: 0.9828 - val_loss: 0.8816 - val_accuracy: 0.9451\n",
            "Epoch 256/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9943\n",
            "Epoch 00256: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 643ms/step - loss: 0.0494 - accuracy: 0.9943 - val_loss: 0.6909 - val_accuracy: 0.9560\n",
            "Epoch 257/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9902\n",
            "Epoch 00257: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 26s 656ms/step - loss: 0.0860 - accuracy: 0.9902 - val_loss: 0.8190 - val_accuracy: 0.9560\n",
            "Epoch 258/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9926\n",
            "Epoch 00258: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 654ms/step - loss: 0.1001 - accuracy: 0.9926 - val_loss: 0.7722 - val_accuracy: 0.9396\n",
            "Epoch 259/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9975\n",
            "Epoch 00259: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 644ms/step - loss: 0.0503 - accuracy: 0.9975 - val_loss: 0.8946 - val_accuracy: 0.9451\n",
            "Epoch 260/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9935\n",
            "Epoch 00260: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.0546 - accuracy: 0.9935 - val_loss: 0.7448 - val_accuracy: 0.9505\n",
            "Epoch 261/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9959\n",
            "Epoch 00261: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.0265 - accuracy: 0.9959 - val_loss: 0.7326 - val_accuracy: 0.9615\n",
            "Epoch 262/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9918\n",
            "Epoch 00262: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 652ms/step - loss: 0.0586 - accuracy: 0.9918 - val_loss: 1.0271 - val_accuracy: 0.9286\n",
            "Epoch 263/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9935\n",
            "Epoch 00263: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 26s 655ms/step - loss: 0.0619 - accuracy: 0.9935 - val_loss: 0.6734 - val_accuracy: 0.9560\n",
            "Epoch 264/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9943\n",
            "Epoch 00264: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.0781 - accuracy: 0.9943 - val_loss: 0.8073 - val_accuracy: 0.9670\n",
            "Epoch 265/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9967\n",
            "Epoch 00265: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.0148 - accuracy: 0.9967 - val_loss: 1.3487 - val_accuracy: 0.9505\n",
            "Epoch 266/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9869\n",
            "Epoch 00266: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.0911 - accuracy: 0.9869 - val_loss: 1.2052 - val_accuracy: 0.9451\n",
            "Epoch 267/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9918\n",
            "Epoch 00267: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0615 - accuracy: 0.9918 - val_loss: 0.8271 - val_accuracy: 0.9615\n",
            "Epoch 268/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9951\n",
            "Epoch 00268: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.0736 - accuracy: 0.9951 - val_loss: 0.9689 - val_accuracy: 0.9451\n",
            "Epoch 269/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9935\n",
            "Epoch 00269: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.1208 - accuracy: 0.9935 - val_loss: 1.4534 - val_accuracy: 0.9286\n",
            "Epoch 270/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9943\n",
            "Epoch 00270: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.0271 - accuracy: 0.9943 - val_loss: 0.8671 - val_accuracy: 0.9505\n",
            "Epoch 271/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9935\n",
            "Epoch 00271: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 649ms/step - loss: 0.0783 - accuracy: 0.9935 - val_loss: 1.5535 - val_accuracy: 0.9231\n",
            "Epoch 272/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9926\n",
            "Epoch 00272: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.1041 - accuracy: 0.9926 - val_loss: 0.9062 - val_accuracy: 0.9505\n",
            "Epoch 273/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9943\n",
            "Epoch 00273: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 642ms/step - loss: 0.0578 - accuracy: 0.9943 - val_loss: 0.9759 - val_accuracy: 0.9615\n",
            "Epoch 274/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9869\n",
            "Epoch 00274: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.1282 - accuracy: 0.9869 - val_loss: 0.6981 - val_accuracy: 0.9780\n",
            "Epoch 275/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9943\n",
            "Epoch 00275: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.0448 - accuracy: 0.9943 - val_loss: 0.9330 - val_accuracy: 0.9615\n",
            "Epoch 276/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9837\n",
            "Epoch 00276: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 642ms/step - loss: 0.1830 - accuracy: 0.9837 - val_loss: 0.6308 - val_accuracy: 0.9560\n",
            "Epoch 277/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9910\n",
            "Epoch 00277: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.1040 - accuracy: 0.9910 - val_loss: 0.6866 - val_accuracy: 0.9725\n",
            "Epoch 278/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9918\n",
            "Epoch 00278: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.1183 - accuracy: 0.9918 - val_loss: 0.5136 - val_accuracy: 0.9780\n",
            "Epoch 279/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9943\n",
            "Epoch 00279: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.0712 - accuracy: 0.9943 - val_loss: 0.8197 - val_accuracy: 0.9615\n",
            "Epoch 280/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9910\n",
            "Epoch 00280: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 638ms/step - loss: 0.0891 - accuracy: 0.9910 - val_loss: 1.2643 - val_accuracy: 0.9396\n",
            "Epoch 281/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9894\n",
            "Epoch 00281: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 643ms/step - loss: 0.0963 - accuracy: 0.9894 - val_loss: 0.6614 - val_accuracy: 0.9670\n",
            "Epoch 282/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9984\n",
            "Epoch 00282: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 648ms/step - loss: 0.0121 - accuracy: 0.9984 - val_loss: 0.6040 - val_accuracy: 0.9780\n",
            "Epoch 283/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9926\n",
            "Epoch 00283: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 642ms/step - loss: 0.0843 - accuracy: 0.9926 - val_loss: 1.1290 - val_accuracy: 0.9505\n",
            "Epoch 284/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9943\n",
            "Epoch 00284: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0356 - accuracy: 0.9943 - val_loss: 0.6660 - val_accuracy: 0.9725\n",
            "Epoch 285/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9845\n",
            "Epoch 00285: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 632ms/step - loss: 0.1787 - accuracy: 0.9845 - val_loss: 0.7753 - val_accuracy: 0.9670\n",
            "Epoch 286/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9877\n",
            "Epoch 00286: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.1554 - accuracy: 0.9877 - val_loss: 0.8911 - val_accuracy: 0.9670\n",
            "Epoch 287/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9918\n",
            "Epoch 00287: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.0451 - accuracy: 0.9918 - val_loss: 0.5933 - val_accuracy: 0.9615\n",
            "Epoch 288/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9910\n",
            "Epoch 00288: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.0638 - accuracy: 0.9910 - val_loss: 0.9040 - val_accuracy: 0.9615\n",
            "Epoch 289/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9959\n",
            "Epoch 00289: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.0740 - accuracy: 0.9959 - val_loss: 0.6958 - val_accuracy: 0.9725\n",
            "Epoch 290/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9869\n",
            "Epoch 00290: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 638ms/step - loss: 0.1309 - accuracy: 0.9869 - val_loss: 0.7446 - val_accuracy: 0.9725\n",
            "Epoch 291/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9951\n",
            "Epoch 00291: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 640ms/step - loss: 0.0887 - accuracy: 0.9951 - val_loss: 1.0650 - val_accuracy: 0.9560\n",
            "Epoch 292/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9739\n",
            "Epoch 00292: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 640ms/step - loss: 0.2763 - accuracy: 0.9739 - val_loss: 1.5854 - val_accuracy: 0.9341\n",
            "Epoch 293/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.5237 - accuracy: 0.9583\n",
            "Epoch 00293: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 636ms/step - loss: 0.5237 - accuracy: 0.9583 - val_loss: 2.4531 - val_accuracy: 0.8626\n",
            "Epoch 294/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9812\n",
            "Epoch 00294: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.1892 - accuracy: 0.9812 - val_loss: 0.8812 - val_accuracy: 0.9615\n",
            "Epoch 295/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9943\n",
            "Epoch 00295: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 644ms/step - loss: 0.0445 - accuracy: 0.9943 - val_loss: 0.7796 - val_accuracy: 0.9670\n",
            "Epoch 296/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9910\n",
            "Epoch 00296: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 644ms/step - loss: 0.1243 - accuracy: 0.9910 - val_loss: 1.4583 - val_accuracy: 0.9286\n",
            "Epoch 297/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9935\n",
            "Epoch 00297: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 650ms/step - loss: 0.0801 - accuracy: 0.9935 - val_loss: 0.7826 - val_accuracy: 0.9670\n",
            "Epoch 298/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9959\n",
            "Epoch 00298: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 650ms/step - loss: 0.0222 - accuracy: 0.9959 - val_loss: 0.7217 - val_accuracy: 0.9725\n",
            "Epoch 299/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9918\n",
            "Epoch 00299: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 649ms/step - loss: 0.0665 - accuracy: 0.9918 - val_loss: 0.8953 - val_accuracy: 0.9615\n",
            "Epoch 300/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9943\n",
            "Epoch 00300: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 645ms/step - loss: 0.0442 - accuracy: 0.9943 - val_loss: 0.7214 - val_accuracy: 0.9615\n",
            "Epoch 301/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9967\n",
            "Epoch 00301: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 650ms/step - loss: 0.0322 - accuracy: 0.9967 - val_loss: 0.9559 - val_accuracy: 0.9505\n",
            "Epoch 302/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9886\n",
            "Epoch 00302: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 26s 654ms/step - loss: 0.1535 - accuracy: 0.9886 - val_loss: 0.6896 - val_accuracy: 0.9670\n",
            "Epoch 303/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9975\n",
            "Epoch 00303: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 638ms/step - loss: 0.0274 - accuracy: 0.9975 - val_loss: 1.4906 - val_accuracy: 0.9231\n",
            "Epoch 304/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9861\n",
            "Epoch 00304: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 648ms/step - loss: 0.1422 - accuracy: 0.9861 - val_loss: 0.7159 - val_accuracy: 0.9615\n",
            "Epoch 305/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9853\n",
            "Epoch 00305: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 645ms/step - loss: 0.1399 - accuracy: 0.9853 - val_loss: 0.7132 - val_accuracy: 0.9451\n",
            "Epoch 306/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9943\n",
            "Epoch 00306: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 638ms/step - loss: 0.0686 - accuracy: 0.9943 - val_loss: 0.7793 - val_accuracy: 0.9396\n",
            "Epoch 307/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9959\n",
            "Epoch 00307: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 642ms/step - loss: 0.1562 - accuracy: 0.9959 - val_loss: 0.6721 - val_accuracy: 0.9560\n",
            "Epoch 308/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9926\n",
            "Epoch 00308: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 643ms/step - loss: 0.0964 - accuracy: 0.9926 - val_loss: 1.1910 - val_accuracy: 0.9341\n",
            "Epoch 309/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9959\n",
            "Epoch 00309: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 640ms/step - loss: 0.0432 - accuracy: 0.9959 - val_loss: 0.7827 - val_accuracy: 0.9560\n",
            "Epoch 310/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9943\n",
            "Epoch 00310: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.1150 - accuracy: 0.9943 - val_loss: 1.6863 - val_accuracy: 0.9286\n",
            "Epoch 311/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9943\n",
            "Epoch 00311: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 633ms/step - loss: 0.1029 - accuracy: 0.9943 - val_loss: 1.4162 - val_accuracy: 0.9505\n",
            "Epoch 312/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9918\n",
            "Epoch 00312: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 629ms/step - loss: 0.1060 - accuracy: 0.9918 - val_loss: 0.7406 - val_accuracy: 0.9670\n",
            "Epoch 313/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9943\n",
            "Epoch 00313: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.0426 - accuracy: 0.9943 - val_loss: 0.7757 - val_accuracy: 0.9670\n",
            "Epoch 314/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9926\n",
            "Epoch 00314: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 26s 655ms/step - loss: 0.0550 - accuracy: 0.9926 - val_loss: 0.7122 - val_accuracy: 0.9670\n",
            "Epoch 315/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9951\n",
            "Epoch 00315: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 648ms/step - loss: 0.0794 - accuracy: 0.9951 - val_loss: 1.4330 - val_accuracy: 0.9451\n",
            "Epoch 316/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9959\n",
            "Epoch 00316: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 647ms/step - loss: 0.0533 - accuracy: 0.9959 - val_loss: 1.2683 - val_accuracy: 0.9231\n",
            "Epoch 317/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9910\n",
            "Epoch 00317: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 641ms/step - loss: 0.1073 - accuracy: 0.9910 - val_loss: 0.7223 - val_accuracy: 0.9725\n",
            "Epoch 318/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9967\n",
            "Epoch 00318: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.0630 - accuracy: 0.9967 - val_loss: 0.8483 - val_accuracy: 0.9670\n",
            "Epoch 319/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9959\n",
            "Epoch 00319: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.0560 - accuracy: 0.9959 - val_loss: 1.1072 - val_accuracy: 0.9560\n",
            "Epoch 320/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9886\n",
            "Epoch 00320: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 640ms/step - loss: 0.1292 - accuracy: 0.9886 - val_loss: 1.5303 - val_accuracy: 0.9121\n",
            "Epoch 321/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9926\n",
            "Epoch 00321: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 639ms/step - loss: 0.0662 - accuracy: 0.9926 - val_loss: 1.1501 - val_accuracy: 0.9451\n",
            "Epoch 322/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9943\n",
            "Epoch 00322: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 641ms/step - loss: 0.0359 - accuracy: 0.9943 - val_loss: 1.3969 - val_accuracy: 0.9505\n",
            "Epoch 323/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9959\n",
            "Epoch 00323: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 645ms/step - loss: 0.0356 - accuracy: 0.9959 - val_loss: 1.3096 - val_accuracy: 0.9505\n",
            "Epoch 324/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9935\n",
            "Epoch 00324: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 637ms/step - loss: 0.0875 - accuracy: 0.9935 - val_loss: 1.1446 - val_accuracy: 0.9341\n",
            "Epoch 325/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2380 - accuracy: 0.9788\n",
            "Epoch 00325: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 0.2380 - accuracy: 0.9788 - val_loss: 0.9898 - val_accuracy: 0.9670\n",
            "Epoch 326/500\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9943\n",
            "Epoch 00326: val_accuracy did not improve from 0.97802\n",
            "39/39 [==============================] - 25s 634ms/step - loss: 0.1719 - accuracy: 0.9943 - val_loss: 1.1200 - val_accuracy: 0.9451\n",
            "Epoch 327/500\n",
            "28/39 [====================>.........] - ETA: 6s - loss: 0.1034 - accuracy: 0.9911"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-21-530af66cbbfc>\", line 11, in <module>\n",
            "    callbacks=callbacks_list\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
            "    return method(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
            "    tmp_logs = train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
            "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
            "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\n",
            "    self.captured_inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\n",
            "    ctx, args, cancellation_manager=cancellation_manager))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 598, in call\n",
            "    ctx=ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
            "    inputs, attrs, num_outputs)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 732, in getmodule\n",
            "    for modname, module in sys.modules.copy().items():\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTZOedcqa4T5"
      },
      "source": [
        "# save it as a h5 file\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# model.save('model_resnet.h5')\n",
        "model = load_model('/content/drive/MyDrive/in_resnet_weights-improvement-153-0.98.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKx7QCg0a4T5"
      },
      "source": [
        "y_pred = model.predict(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk1XDpNJa4T6",
        "outputId": "3ddffe2f-0e5e-43a6-cc1c-ade5e3b985a1"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.000000e+00, 1.000000e+00],\n",
              "       [5.412582e-34, 1.000000e+00],\n",
              "       [0.000000e+00, 1.000000e+00],\n",
              "       ...,\n",
              "       [0.000000e+00, 1.000000e+00],\n",
              "       [0.000000e+00, 1.000000e+00],\n",
              "       [1.000000e+00, 0.000000e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOzXRP-8a4T6"
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loHHb7RNIwNY",
        "outputId": "aceeac92-9c7f-40e1-9e6e-487265892b39"
      },
      "source": [
        "list1 = ['ad','bd','cd','dd','ed']\n",
        "#for i in list1:\n",
        "  #print(i)\n",
        "\n",
        "#for j in list1:\n",
        "d = [j.split(\",\") for j in list1]\n",
        "d   \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ad'], ['bd'], ['cd'], ['dd'], ['ed']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICc4F4Aha4T6",
        "outputId": "64fe53c0-55b5-4d18-9646-79a3ce4ed6b4"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cQEhku0a4T7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "45b18941-a945-4746-84f8-e1a29dd21186"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "# plot the loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "\n",
        "# plot the accuracy\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTY-aqiaZPj6"
      },
      "source": [
        "# load_model_sample.py\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def load_image(img_path, show=False):\n",
        "\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
        "    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    CATEGORIES = [\"Grouped\", \"Simple\", \"Stacked\"]\n",
        "    # load model\n",
        "    #model = load_model(\"/content/drive/MyDrive/in_rs_weights-improvement-291-0.85.hdf5\")\n",
        "\n",
        "    # image path\n",
        "    img_path = '/content/simp3.png'\n",
        "\n",
        "    # load a single image\n",
        "    new_image = load_image(img_path)\n",
        "\n",
        "    # check prediction\n",
        "    pred = model.predict(new_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R7zsfKE7ZZn5",
        "outputId": "7490b4d5-be85-4c56-cbf8-bf60ec72cec4"
      },
      "source": [
        "import numpy as np\n",
        "#This function returns indices of the maximum values are returned along with the specified axis\n",
        "#pred = np.argmax(pred)\n",
        "pred = CATEGORIES[np.argmax(pred)]\n",
        "\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Simple'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}